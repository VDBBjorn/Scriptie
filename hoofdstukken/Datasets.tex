\chapter{Datasets}\label{hs:datasets}
Om het systeem te testen wordt gebruik gemaakt van twee datasets: Het Laatste Nieuws en Wikipedia. Beide documentencollecties zijn opgesteld in het Nederlands. Voor elke dataset wordt telkens de verdeling en het totale aantal artikels vermeld.

\section{Het Laatste Nieuws}
De datasets die in eerste instantie gebruikt zullen worden zijn afkomstig van Het Laatste Nieuws\footnote{http://hln.be}, een populaire online en offline krant in Belgi\"e. De artikels die gebruikt worden komen van de online krant en werden gedownload via het archief. Om rekenkracht te besparen tijdens het onderzoek worden twee datasets gebruikt; een dataset met alle artikels van januari 2008 (10126 artikels) en een dataset met alle artikels van het jaar 2008 (82753 artikels). De dataset van januari 2008 doet tevens dienst als trainsset voor de verschillende classifiers in sectie \ref{classificatie}.

\begin{table}[htbp]
	\centering
	\caption{Verdeling van artikels over de verschillende hoofdcategorie\"en voor dataset Het Laatste Nieuws, januari 2008}
	\begin{tabular}{rrr}
		\toprule
		Categorie & Aantal & Gewicht \\
		\midrule
		Nieuws & 3209  & 31.69\% \\
		You   & 2952  & 29.15\% \\
		Sport & 1381  & 13.64\% \\
		Geld  & 918   & 9.07\% \\
		Showbizz & 575   & 5.68\% \\
		Reizen & 369   & 3.64\% \\
		Bizar & 353   & 3.49\% \\
		iHLN  & 128   & 1.26\% \\
		Wetenschap & 114   & 1.13\% \\
		Auto  & 99    & 0.98\% \\
		Muziek & 28    & 0.28\% \\
		\textbf{Totaal} & \textbf{10126} &\textbf{ 100.00\%} \\
		\bottomrule
	\end{tabular}%
	\label{tab:hln-2008-01-cat}%
\end{table}%

Elk artikel behoort toe aan een boomstructuur van categorie\"en met twee niveaus. Het aantal hoofdcategorie\"en varieert tussen elf en twaalf (tijdens 2008 werd 1 hoofdcategorie toegevoegd in vergelijking met januari 2008). Voor de dataset van januari 2008 kunnen de hoofdcategorie\"en met hun verdeling teruggevonden worden in tabel \ref{tab:hln-2008-01-cat}. De dataset voor het volledige jaar 2008 kan teruggevonden worden in tabel \ref{tab:hln-2008-cat}. Merk op dat de categorie "Planet" werd ge\"introduceerd (aandeel 1.36\%).

De data werd opgehaald met een zelfgeschreven Perl\footnote{https://www.perl.org/} scraper gekoppeld aan de RSS-feed van Het Laatste Nieuws. Deze selecteert de body van elk artikel m.b.v. een reguliere expressie. Deze documenten worden vervolgens opgeslagen onder hun respectievelijke categorie, zodat ze bruikbaar zijn als input voor content recognition systemen. 

\begin{table}[htbp]
	\centering
	\caption{Verdeling van artikels over de verschillende hoofdcategorie\"en voor dataset Het Laatste Nieuws, jaar 2008}
	\begin{tabular}{rrr}
		\toprule
		Categorie & Aantal & Gewicht \\
		\midrule
		Nieuws & 35436 & 42.82\% \\
		Sport & 16168 & 19.54\% \\
		Showbizz & 6687  & 8.08\% \\
		Geld  & 6684  & 8.08\% \\
		You   & 5739  & 6.94\% \\
		Bizar & 4616  & 5.58\% \\
		Reizen & 2670  & 3.23\% \\
		iHLN  & 1141  & 1.38\% \\
		Planet & 1128  & 1.36\% \\
		Auto  & 1010  & 1.22\% \\
		Wetenschap & 1008  & 1.22\% \\
		Muziek & 466   & 0.56\% \\
		\textbf{Totaal} & \textbf{82753} & \textbf{100.00\%} \\
		\bottomrule
	\end{tabular}%
	\label{tab:hln-2008-cat}%
\end{table}%

\section{Wikipedia}
Wikipedia\footnote{https://nl.wikipedia.org/} is een gemeenschapsproject met als doel in elke taal een complete encyclopedie op het internet te cre\"eren. Om deze dataset op te stellen werden voor elke hoofdcategorie 10000 artikels gedownload, die evenredig verdeeld zijn over de onderliggende subcategorie\"en. Dit maakt dat er in totaal 90000 artikels zijn. Wikipedia heeft een veel grotere boomstructuur met meer subcategorie\"en dan Het Laatste Nieuws. Om de vergelijking met de andere dataset mogelijk te maken, werden de artikels  gecategoriseerd onder de bovenste twee niveaus. Op die manier krijgt elk artikel op Wikipedia een indeling in een hoofd- en subcategorie. 

Om bepaalde testen uit te voeren wordt nog een tweede subset (trainingsset) gemaakt van deze dataset, waarbij telkens 1000 artikels per hoofdcategorie opgenomen zijn in de dataset (en dus in totaal 9000 artikels).

Ook de Wikipedia dataset werd opgehaald met een zelfgeschreven Perl scraper. Deze haakt in op de API die wikipedia aanbiedt om de artikels relevant voor bepaalde categorie\"en op te halen. 