\chapter{Overzicht werking}\label{hs:werking}

\section{Algemeen}
Gebruik Java/Perl/... uitleggen

\section{WEKA}
WEKA (Waikato Environment for Knowledge Analysis) is een tool voor datamining ontwikkeld in de programmeertaal Java. Het bestaat uit een grafische werkomgeving  voor het uitvoeren van de nodige stappen bij datamining, een CLI en is volledig ondersteund voor gebruik binnen een eigen Java-programma. Het helpt bij de preprocessing van data en bevat heel wat algoritmes voor clustering, classificatie, regressie-analyse, visualisatie en feature selection.

De versie van WEKA die gebruikt wordt in deze masterproef is 3.6.13, de op heden laatste stabiele versie.

\textbf{TODO: meer + referentie}

\section{Metadata extractie met Apache Tika}
Apache tika wordt aangeboden onder de vorm van een REST-API, gebouwd op het Jersey-framework in Java. Het Jersey-framework is gebaseerd op JAX-RS en laat toe om verschillende types data aan te bieden aan de gebruiker. Het wordt gedeployed naar een Glassfish-omgeving, die vervolgens kan worden aangesproken door gebruikers van de API.

Op zich bevat de API slechts twee verschillende \textit{endpoints}; \quotes{/test} controleert of de API wel degelijk reageert op calls, terwijl \quotes{/upload} een document aanvaardt in de vorm van een tekst en die gaat parsen naar platte tekst. Het parsen bestaat uit twee delen. Het eerste deel is de taaldetectie, welke door de standaard \textit{LanguageIdentifier} van het Tika framework wordt uitgevoerd. Het tweede deel vervolgens is het effectief parsen van de tekst. Hiervoor wordt de \textit{AutoDetectParser} van Tika gebruikt. Deze kiest voor elk document de parser die voor dat type het beste resultaat zal opleveren.
De resultaten van de parser worden teruggestuurd naar de initiator van de connectie in de vorm van JSON:

\begin{lstlisting}
{
"language":"nl",
"title":"5-000-deelnemers-opname-clip-klimaatsverandering\n",
"body":"Op het Klein Strand in Oostende zijn vanmiddag naar schatting 5.000 mensen samengekomen op een klimaathappening die onderdeel was van de internationale campagne 'The Big Ask'. De deelnemers figureerden in een filmpje van regisseur Nic Baltazar en Friends of the Earth vzw.Filmpje YoutubeOp het Klein Strand vormden de deelnemers letters en slogans die vanuit de hoogte werden opgenomen en later zullen te zien zijn in een filmpje op Youtube. De actie 'Sos Klimaat : bouw een dijk tegen klimaatsverandering' kreeg de steun van talrijke BV's die als dj de massa animeerden. Onder meer Zohra, Flip Kowlier, Gabriel Rios en Adriaan Van den Hoof verleenden op het podium hun medewerking.Politici wakkerschuddenInitiatiefnemer Nic Baltazar was in elk geval tevreden met de opkomst en de opnames en verwacht dat de clip op Youtube voor een half miljoen hits zal zorgen. Hij hoopt dat de actie en de clip politici zal wakker schudden omtrent de problematiek. (belga/ep)\n"
}
\end{lstlisting} 

\section{Preprocessing van de documentencollectie}\label{preprocessing}
De datasets van Het Laatste Nieuws bevatten, afhankelijk van de tijdsperiode, 11 tot 12 hoofdcategorie\"en. Voor de dataset van januari 2008 zijn deze hoofdcategorie\"en met hun aandeel in het totale aantal artikels te vinden in tabel \ref{tab:hln-2008-01-cat}. Elke hoofdcategorie bevat bovendien een aantal subcategorie\"en. Elk artikel behoort bijgevolg toe aan twee categorie\"en. 

De eerste stap, voor we de classificatie en clustering toepassen, omvat het preprocessen van alle documenten uit de documentencollectie. In WEKA wordt hiervoor de functie \textit{string-to-wordvector} gebruikt. Deze functie converteert de teksten naar een set van attributen die het aandeel van dat attribuut voorstellen in de documentencollectie (zie \ref{vector-space-model}). 

De string-to-word functie in WEKA neemt een aantal parameters. Een eerste parameter is de tokenizer functie. Deze bepaalt hoe een tekst wordt opgesplitst in woorden. Hier wordt gebruik gemaakt van een N-Gram tokenizer. Deze geeft de mogelijkheid om eventueel twee of drie woorden samen te nemen en als \'e\'en woord in de documentencollectie te zien.

Ten tweede kan gekozen worden voor term frequency-inverse document frequency (tf-idf), al dan niet gecombineerd met een normalisatie. De achterliggende logica zit vervat in formule \ref{eq:tfidf}. Op deze manier worden termen die belangrijk zijn voor een document, maar minder vaak of niet in de volledige documentencollectie voorkomen, een groter gewicht toegekend in de vector representatie. 

Een derde parameter die ingesteld kan worden is de het gebruikte stemmingsalgoritme. Hier werd een Nederlandstalige stemmer gebruikt, zoals beschreven in \cite{Kraaij1994}. Deze stemmer is een uitbreiding van het originele stemmingsalgoritme van Porter, zoals voorgesteld in \cite{Porter1980}, naar de Nederlandse taal. 

Een vierde parameter is de stopwoordfilter. Hiervoor wordt een lijst met stopwoorden gebruikt voor de Nederlandse taal. De stopwoordfilter zorgt ervoor dat woorden die veel voorkomen in een taal, maar relatief weinig inhoud hebben, niet overwogen worden om op te nemen in het woordenboek. Denk daarbij aan 'de', 'het', 'daar', 'dan', etc. Na het opsplitsen van de zin naar woorden (tokenization, zie \ref{tokenization}) kunnen deze woorden weggefilterd worden, zodat ze geen negatieve invloed hebben op het uiteindelijk resultaat.

Een vijfde parameter laat toe om de frequentie van een woord uit te drukken, in plaats van binaire aanwezigheid. Hierdoor wordt de belangrijkheid van bepaalde woorden soms meer in de verf gezet. Wel is het opletten dat sommige minder belangrijke woorden zo niet een grotere rol krijgen dan ze eigenlijk verdienen.

De laatste parameter is het aantal woorden dat behouden wordt per klasse uit de documentencollectie. Dit is natuurlijk een belangrijke parameter, omdat meer betekenisvolle woorden in het woordenboek naar een betere classificatie en/of clustering zullen leiden. Als we echter te veel woorden opnemen, waardoor er een bepaalde ruis gaat optreden, krijgen we ook slechts een suboptimale werking. Het is echter moeilijk om zomaar te bepalen hoeveel woorden een optimale classificatie zullen geven bij elke methode. Bovendien is het mogelijk dat het voor onze dataset niet nuttig is om een stemmer of een stopwoordfilter te gaan gebruiken. Daarom wordt in \ref{result-clas} een benchmark opgesteld die het optimale aantal woorden gaat onderzoeken bij classificatie. Verder wordt ook het  nut van de stemmer en van de stopwoordfilter verder onderzocht. 

\textbf{TODO: hoe doen we dit bij clustering? zelfde?\\-- wrs niet, wel nood aan tf-idf, normalisatie, word count (freq)}

\section{Classificatie}\label{result-clas}




\section{Clustering}
Voor clustering wordt gebruikt gemaakt van het "K-means$"$ algoritme en het "Expectation Maximization" (EM) algoritme. 

\section{Aanbevelingssysteem}