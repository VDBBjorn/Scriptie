\chapter{Overzicht werking}\label{hs:werking}

In het vorige hoofdstuk werd de architectuur van het systeem geschetst. In dit hoofdstuk wordt verder ingegaan op de effectieve implementatie van het systeem en de eventueel andere overwogen opties. Voor alle back-end oplossingen werd gekozen voor de programmeertaal Java\footnote{http://www.java.com/}. Java is een krachtige programmeertaal die bruikbaar is in veel domeinen. 

\section{Voorbereiden van de dataset}
\subsection{Verwerking met Apache Tika}
Om met verschillende documentencollecties te kunnen werken binnen het ontworpen systeem, wordt gebruik gemaakt van Apache Tika \cite{tika}. Tika laat toe om metadata van een tekst af te leiden. Die metadata kan o.a. de taal of het formaat van het document in kwestie zijn. Deze data kan belangrijk zijn bij de keuze van het stemmingsalgoritme dat gebruikt moet worden (dit is afhankelijk van de taal, zie \ref{stemming}) of laat toe om verschillende soorten documentformaten (pdf, doc, docx, csv, html, etc) om te vormen tot platte tekst zonder franjes. Deze tekst kan veel beter ge\"interpreteerd worden door een computer.

Het parsen van een document bestaat uit twee delen. Het eerste deel is de taaldetectie, welke door de standaard \textit{LanguageIdentifier} van het Tika framework wordt uitgevoerd. Het tweede deel is het effectief parsen van de tekst. Hiervoor wordt de \textit{AutoDetectParser} van Tika gebruikt. Deze kiest voor elk document de parser die voor dat type het beste resultaat zal opleveren.
De resultaten van de parser komen in de vorm van JSON. Dit principe wordt in listing \ref{lst:plaintext} ge\"illustreerd a.d.h.v. een item van Het Laatste Nieuws, waarbij de tekst werd doorgegeven als html-bestand. Zowel de taal als de tekst wordt perfect uit het html-bestand gehaald.

\begin{lstlisting}[caption=bjorn,label=lst:plaintext]
{
"language":"nl",
"title":"5000 deelnemers opname clip klimaatsverandering\n",
"body":"Op het Klein Strand in Oostende zijn vanmiddag naar schatting 5.000 mensen samengekomen op een klimaathappening die onderdeel was van de internationale campagne 'The Big Ask'. De deelnemers figureerden in een filmpje van regisseur Nic Baltazar en Friends of the Earth vzw.Filmpje YoutubeOp het Klein Strand vormden de deelnemers letters en slogans die vanuit de hoogte werden opgenomen en later zullen te zien zijn in een filmpje op Youtube. De actie 'Sos Klimaat : bouw een dijk tegen klimaatsverandering' kreeg de steun van talrijke BV's die als dj de massa animeerden. Onder meer Zohra, Flip Kowlier, Gabriel Rios en Adriaan Van den Hoof verleenden op het podium hun medewerking.Politici wakkerschudden Initiatiefnemer Nic Baltazar was in elk geval tevreden met de opkomst en de opnames en verwacht dat de clip op Youtube voor een half miljoen hits zal zorgen. Hij hoopt dat de actie en de clip politici zal wakker schudden omtrent de problematiek. (belga/ep)\n"
}
\end{lstlisting} 

\subsection{Preprocessing van tekstdocumenten}\label{bag-of-words}\label{preprocessing}
De platte tekst in de documentencollectie bevat op zich te weinig structuur om effici\"ent verwerkt te worden door een computer. Om de data beter te kunnen begrijpen moeten we de dataset eerst omvormen tot een feature vector die vervolgens als input kan dienen voor content recognition systemen. De noden naar preprocessing zijn voor zowel classificatie als clustering zeer gelijklopend. Toch wordt voor classificatie een extra stap toegevoegd die probeert de dimensionaliteit van de features te verlagen (zie \ref{feature-selection}). Deze stap wordt niet voor clustering toegepast omdat de verwijderde features mogelijk wel nuttig kunnen zijn voor clustering.

\subsubsection{Tokenization}
Om alle woorden te verkrijgen die gebruikt worden in een bepaalde tekst, wordt een \textit{tokenization}\label{tokenization} proces toegepast. Dit proces Dit zorgt ervoor dat een tekstdocument gesplitst wordt in een stroom van woorden door alle leestekens te verwijderen en alle tabs en niet-tekstuele karakters door spaties te vervangen. De set van verschillende woorden uit alle tekstdocumenten wordt samengevoegd tot het woordenboek van de documentencollectie. 

Het is echter ook mogelijk om een N-Gram tokenizer \cite{McNamee2004} te gebruiken. Deze geeft de mogelijkheid om eventueel twee of drie woorden samen te nemen en die als \'e\'en woord in onze documentencollectie te zien. Dit kan voordeel geven bij o.a. personennamen, welke vaak bestaan uit twee woorden die voor een tekst belangrijk zouden kunnen zijn. Uiteraard wordt een eigennaam ook opgepikt indien alle woorden apart beschouwd worden.

Om de grootte van het woordenboek en dus de dimensionaliteit van de beschrijving van de documentencollectie te verkleinen, wordt de set van woorden verder gereduceerd door het toepassen van filters of \textit{stemming}salgoritmes. 

\subsubsection{Filter- en stemmingsalgoritmes}\label{stemming}
Filters verwijderen woorden van het woordenboek en dus uit de documenten. De filtering die het meest wordt toegepast op tekstuele collecties is stopwoordfiltering. De stopwoorden die verwijderd worden zijn woorden die weinig of geen inhoud hebben. Voorbeelden zijn lidwoorden, verbindingswoorden, voorzetsels, etc. \\
Stemmingsalgoritmes proberen een woord om te vormen tot de standaardvorm van dat woord. Dit doen ze bijvoorbeeld door meervouden van zelfstandige naamwoorden naar het enkelvoud om te zetten of door werkwoorden naar hun stam te vereenvoudigen. Het stemmingalgoritme dat hier gebruikt wordt is Porters stemming algoritme voor de Nederlandse taal \cite{Kraaij1994}. Het is een implementatie van Porters stemming algoritme \cite{Porter1980}, dat origineel enkel voor de Engelse taal werd ontworpen.

\subsection{Vector space model}\label{vector-space-model}
Ondanks de simpele datastructuur zorgt het vector space model ervoor dat grote collecties documenten effici\"ent kunnen geanalyseerd worden. Het representeert documenten als vectors in een $m$-dimensionale ruimte. Elk document $d$ uit de documentencollectie is beschreven als een numerieke\textit{ feature vector} $w(d) = (x(d,t_1),...,x(d,t_m))$ waarbij $T=\{t_1,...,t_m\}$ het woordenboek voorstelt. De hoofdtaak van de vector space representatie van documenten is het vinden van een geschikte encodering van de feature vector. 

Elk element van de vector representeert meestal een woord (of groep van woorden) van de documentencollectie. De simpelste manier om een document te encoderen is om \textit{binary term} vectoren te gebruiken. Als een woord voorkomt in het document wordt het corresponderende element op \'e\'en gezet, komt het niet voor dan is het nul. De encodering wordt zo herleid tot een simpele Booleaanse vergelijking. Hierbij wordt de belangrijkheid van elk woord als gelijkwaardig beschouwd. 

Om de performantie te verbeteren kunnen \textit{term weighting schemes} worden gebruikt \cite{Salton1988}. Het gewicht dat toegekend wordt aan een woord reflecteert de belangrijkheid of relevantie van dat woord in een specifiek document of collectie. Een woord met hoge frequentie in bepaalde documenten, maar dat weinig of niet voorkomt in de volledige documentencollectie wordt een groot gewicht toebedeeld. Een gewicht $w(d,t)$ voor term $t$ in document $d$ wordt berekend als de term frequency $tf(d,t)$ vermenigvuldigd met de inverse document frequency $idf(t)$ - gedefinieerd als $idf(t)=\log{\frac{N}{n_t}})$. Dit beschrijft de specifiteit van een bepaalde term in een documentencollectie. 

Naast term frequency en inverse document frequency wordt een normalisatie toegepast om ervoor te zorgen dat alle documenten dezelfde kans hebben om gevonden te worden, onafhankelijk van hun lengte. Deze techniek heeft zijn nut reeds bewezen in de praktijk . 
\begin{equation}\label{eq:tfidf}
w(d,t) = \frac{tf(dt)\log{\frac{N}{N_t}}}{\sqrt{\sum_{j=1}^{m}tf(d,t_j)^2(log(\frac{N}{n_{t_j}}))^2}}
\end{equation}
Hierbij stelt $N$ de grootte van de documentencollectie $D$ voor en is $n_t$ het aantal documenten in $D$ dat term $t$ bevat.

\subsection{Feature selection}\label{feature-selection}
Feature selection of attribute selection is een proces dat enkel bij classificatie doorlopen wordt. Het zorgt ervoor dat attributen die niet relevant zijn verwijderd worden. Zo wordt een subset van de features gezocht die relevant zijn voor het doelconcept. Dit zorgt er o.a. voor dat de dataset kleiner wordt, waardoor minder rekenkracht en zoekruimte vereist is bij de effectieve verwerking van het vector model. Volgens \cite{Liu2005} is het \'e\'en van de meest belangrijke en meest gebruikte technieken voor data preprocessing bij data mining. Het reduceert het aantal features, het verwijdert irrelevante of redundante data en het zorgt er bijgevolg voor dat data mining algoritmes sneller werken. Het zorgt er o.m. ook voor dat de voorspelde accuraatheid en begrijpbaarheid van de resultaten verbeterd wordt. Feature selection is vooral interessant bij text mining, omdat er een hoge dimensionaliteit is van de features en er veel irrelevante features voorkomen.

Als de dimensionaliteit van een domein bovendien vergroot wordt, verhoogt het aantal features $N$. Een optimale subset van features vinden is zeer moeilijk en veel problemen gerelateerd aan feature selection zijn NP-moeilijk. Volgens \cite{Liu2005} bestaat een typisch feature selection proces uit vier stappen (zie figuur \ref{fig:feature-selection}); subset generatie, subset evaluatie, stopcriterium en resultatenvalidatie.

\begin{figure}[h]
	\caption{Vier stappen in feature selection}
	\label{fig:feature-selection}
	\includegraphics[width=\textwidth]{fig/feature-selection}
\end{figure}

Subset generatie is een zoekprocedure die kandidaat feature subsets produceert voor evaluatie met een bepaalde zoekstrategie. Elke kandidaat subset wordt ge\"evalueerd en vergeleken met de vorige beste volgens een bepaald criterium. Als de nieuwe subset beter is, wordt de vorige beste subset vervangen. Dit proces wordt herhaald tot een bepaald stopcriterium is bereikt. Dan wordt de geselecteerde beste subset gevalideerd met voorkennis of via verschillende tests op andere datasets. 

\section{Classificatie}
Een eerste techniek die toegepast wordt om een aanbevelingssysteem een beter beeld te geven van de documentencollectie, is classificatie. Door een artikel automatisch aan een categorie te kunnen toewijzen, krijgen we al een veel duidelijker beeld van waarover dit artikel nu precies handelt. Als we zowel de hoofd- als subcategorie van een artikel kunnen bepalen, hebben we al een eerste sterke indicatie of het artikel al dan niet interessant kan zijn voor de gebruiker en dus aanbevolen zal worden door het aanbevelingssysteem.

\subsection{WEKA}
Om zoveel mogelijk classificatiemethodes te kunnen testen wordt gebruik gemaakt van WEKA \cite{weka} (Waikato Environment for Knowledge Analysis). WEKA is een tool voor datamining ontwikkeld in de programmeertaal Java. Het bestaat uit een grafische werkomgeving  voor het uitvoeren van de nodige stappen bij datamining, een CLI en is volledig ondersteund voor gebruik binnen een eigen Java-programma. Dit framework bevat in totaal 57 verschillende algoritmes om classificatie door te voeren. De zijn ingedeeld in verschillende categorie\"en: Bayes, functions, lazy, meta, mi, misc, rules en trees.

De versie van WEKA die gebruikt wordt in deze masterproef is 3.6.13.

De verschillende classifiers binnen WEKA ondersteunen geen meerdere niveaus van classificatie. Om toch gebruik te kunnen maken van een boomstructuur worden de artikels eerst ingedeeld in hun respectievelijke hoofdcategorie\"en en pas daarna in de subcategorie\"en. Omdat het mogelijk is dat voor deze gevallen een andere classifier een beter resultaat oplevert, gaan we deze apart testen. De preprocessing blijft voor alle gevallen gelijk. Wel gaan we eerst uitzoeken welke parameters van de preprocessing het beste beeld geven voor enkele na\"ieve classifiers. 

In eerste instantie wordt gefocust op de dataset van Het Laatste Nieuws. Vervolgens wordt met de gevonden inzichten classificatie toegepast op een tweede dataset van Wikipedia. 

\subsection{Preprocessing in WEKA}
In WEKA wordt voor preprocessing van de documentencollectie de functie \textit{string-to-wordvector} gebruikt. Deze functie converteert de teksten naar een set van attributen die het aandeel van dat attribuut voorstellen in de documentencollectie. 

De string-to-word functie in WEKA neemt een aantal parameters. Een eerste parameter is de tokenizer functie. Deze bepaalt hoe een tekst wordt opgesplitst in woorden. Hier wordt gebruik gemaakt van een N-Gram tokenizer. Deze geeft de mogelijkheid om eventueel twee of drie woorden samen te nemen en als \'e\'en woord in de documentencollectie te zien.

Ten tweede kan gekozen worden voor term frequency-inverse document frequency (tf-idf), al dan niet gecombineerd met een normalisatie. De achterliggende logica zit vervat in formule \ref{eq:tfidf}. Op deze manier worden termen die belangrijk zijn voor een document, maar minder vaak of niet in de volledige documentencollectie voorkomen, een groter gewicht toegekend in de vector representatie. 

Een derde parameter die ingesteld kan worden is de het gebruikte stemmingsalgoritme. Hier werd een Nederlandstalige stemmer gebruikt, zoals beschreven in \cite{Kraaij1994}. Deze stemmer is een uitbreiding van het originele stemmingsalgoritme van Porter, zoals voorgesteld in \cite{Porter1980}, naar de Nederlandse taal. 

Een vierde parameter is de stopwoordfilter. Hiervoor wordt een lijst met stopwoorden gebruikt voor de Nederlandse taal. De stopwoordfilter zorgt ervoor dat woorden die veel voorkomen in een taal, maar relatief weinig inhoud hebben, niet overwogen worden om op te nemen in het woordenboek. Denk daarbij aan 'de', 'het', 'daar', 'dan', etc. Na de fase van tokenization kunnen deze woorden weggefilterd worden, zodat ze geen negatieve invloed hebben op het uiteindelijk resultaat.

Een vijfde parameter laat toe om de frequentie van een woord uit te drukken, in plaats van binaire aanwezigheid. Hierdoor wordt de belangrijkheid van bepaalde woorden soms meer in de verf gezet. Wel is het opletten dat sommige minder belangrijke woorden zo niet een grotere rol krijgen dan ze eigenlijk verdienen.

De laatste parameter is het aantal woorden dat behouden wordt per klasse uit de documentencollectie. Dit is natuurlijk een belangrijke parameter, omdat meer betekenisvolle woorden in het woordenboek naar een betere classificatie en/of clustering zullen leiden. Als we echter te veel woorden opnemen, waardoor er een bepaalde ruis gaat optreden, krijgen we ook slechts een suboptimale werking. Het is echter moeilijk om zomaar te bepalen hoeveel woorden een optimale classificatie zullen geven bij elke methode. Bovendien is het mogelijk dat het voor onze dataset niet nuttig is om een stemmer of een stopwoordfilter te gaan gebruiken. Daarom wordt in sectie \ref{parameters-voor-preprocessing} een benchmark opgesteld die het optimale aantal woorden gaat onderzoeken bij classificatie. Verder wordt daar ook het nut van de stemmer en van de stopwoordfilter verder onderzocht. Ook wordt het effect van deze parameters op de uiteindelijke classificatie geanalyseerd. 
In sectie \ref{IG} komen we terug op feature selection. Er wordt onderzocht of deze functie inderdaad een positief effect kan hebben op de effectieve classificatie.

\subsection{Parameters voor preprocessing van data}\label{parameters-voor-preprocessing}

Om de optimale parameters voor de preprocessing te bepalen wordt een classificatie uitgevoerd met de verwerkte dataset. We vari\"eren de parameters en bepalen zo de optimale instellingen. De dataset die eerst gebruikt wordt is deze van Het Laatste Nieuws, januari 2008. De classifiers die gebruikt worden om de bekomen woordvector te testen zijn "Naive Bayes$"$ en "Naive Bayes Multinomial". 

Om een betrouwbaar resultaat te bekomen worden de tests met een \textit{10-fold cross validation}\label{10-fold-cv} uitgevoerd. Dit is een techniek om voorspellende modellen te evalueren door de originele dataset te verdelen in een trainingsset om het model te trainen en een testset om deze te valideren. Bij 10-fold cross-validation\label{cross-val} wordt de originele dataset random verdeeld in 10 subsets. Daarvan wordt \'e\'en subset behouden ter validatie van het model. De overige 9 subsets worden gebruikt als trainingsdata. Het cross-validation proces wordt 10 keer herhaald, waarbij elk van de subsets exact \'e\'en keer gebruikt worden als validatieset. De 10 resultaten worden gecombineerd (uitgemiddeld) om een enkele voorspelling te maken. Het grote voordeel van deze methode is dat alle observaties zowel gebruikt worden als trainings- en validatiedata. 

Om de resultaten onafhankelijk te stellen van het aantal woorden dat behouden wordt van de documentencollectie, gebruiken we de standaard bijgehouden 1000 woorden. Deze parameter wordt per dataset apart geverifieerd. Verder worden de verschillende parameters al dan niet ingesteld, zoals te zien in tabel \ref{tab:preprocessing-params}. Hierbij staat \quotes{1} voor gebruikt en \quotes{0} voor niet gebruikt. 

\newpage
\begin{longtable}{rrrrrrr}
	\caption{Gebruik van verschillende parameters bij preprocessing van de documentencollectie \quotes{Het Laatste Nieuws, januari 2008} en hun effect op het percentage correct geclassificeerde artikels volgens de \quotes{Naive Bayes} (NB) en \quotes{Naive Bayes Multinomial} (NBM) classifiers in hoofdcategorie\"en.}\\
	\toprule
	\multicolumn{1}{c}{\textbf{tf-idf}} & \multicolumn{1}{c}{\textbf{Stemming}} & \multicolumn{1}{c}{\textbf{Stopwoorden}} & \multicolumn{1}{c}{\textbf{Frequentie}} & \multicolumn{1}{c}{\textbf{Tokenizer}} & \multicolumn{1}{c}{\textbf{NB}} & \multicolumn{1}{c}{\textbf{NBM}} \\
	\midrule
	0     & 0     & 0     & 0     & 1     & 69.85\% & 74.39\% \\
	0     & 0     & 0     & 0     & 2     & 67.62\% & 72.34\% \\
	0     & 0     & 0     & 0     & 3     & 67.44\% & 72.15\% \\
	0     & 0     & 0     & 1     & 1     & 56.06\% & 66.39\% \\
	0     & 0     & 0     & 1     & 2     & 54.26\% & 64.35\% \\
	0     & 0     & 0     & 1     & 3     & 54.24\% & 64.30\% \\
	0     & 0     & 1     & 0     & 1     & 71.50\% & 75.71\% \\
	0     & 0     & 1     & 0     & 2     & 69.26\% & 73.41\% \\
	0     & 0     & 1     & 0     & 3     & 68.91\% & 73.09\% \\
	0     & 0     & 1     & 1     & 1     & 58.01\% & 68.31\% \\
	0     & 0     & 1     & 1     & 2     & 56.25\% & 66.18\% \\
	0     & 0     & 1     & 1     & 3     & 56.20\% & 66.09\% \\
	0     & 1     & 0     & 0     & 1     & 70.85\% & 75.43\% \\
	0     & 1     & 0     & 0     & 2     & 68.19\% & 72.65\% \\
	0     & 1     & 0     & 0     & 3     & 68.35\% & 72.61\% \\
	0     & 1     & 0     & 1     & 1     & 57.68\% & 67.55\% \\
	0     & 1     & 0     & 1     & 2     & 55.74\% & 65.36\% \\
	0     & 1     & 0     & 1     & 3     & 55.88\% & 65.43\% \\
	0     & 1     & 1     & 0     & 1     & \textbf{71.91\%} & \textbf{76.37\%} \\
	0     & 1     & 1     & 0     & 2     & 69.44\% & 73.82\% \\
	0     & 1     & 1     & 0     & 3     & 69.42\% & 73.70\% \\
	0     & 1     & 1     & 1     & 1     & 59.09\% & 69.31\% \\
	0     & 1     & 1     & 1     & 2     & 57.49\% & 67.30\% \\
	0     & 1     & 1     & 1     & 3     & 57.29\% & 67.07\% \\
	1     & 0     & 0     & 0     & 1     & 36.40\% & 57.44\% \\
	1     & 0     & 0     & 0     & 2     & 40.78\% & 58.42\% \\
	1     & 0     & 0     & 0     & 3     & 40.85\% & 58.31\% \\
	1     & 0     & 0     & 1     & 1     & 40.88\% & 59.59\% \\
	1     & 0     & 0     & 1     & 2     & 45.99\% & 60.97\% \\
	1     & 0     & 0     & 1     & 3     & 45.68\% & 60.59\% \\
	1     & 0     & 1     & 0     & 1     & 34.45\% & 56.63\% \\
	1     & 0     & 1     & 0     & 2     & 39.73\% & 58.00\% \\
	1     & 0     & 1     & 0     & 3     & 39.73\% & 57.79\% \\
	1     & 0     & 1     & 1     & 1     & 38.56\% & 58.81\% \\
	1     & 0     & 1     & 1     & 2     & 43.64\% & 59.91\% \\
	1     & 0     & 1     & 1     & 3     & 43.97\% & 59.96\% \\
	1     & 1     & 0     & 0     & 1     & 38.92\% & 59.00\% \\
	1     & 1     & 0     & 0     & 2     & 42.52\% & 59.58\% \\
	1     & 1     & 0     & 0     & 3     & 42.72\% & 59.59\% \\
	1     & 1     & 0     & 1     & 1     & 43.99\% & 61.61\% \\
	1     & 1     & 0     & 1     & 2     & 47.16\% & 62.01\% \\
	1     & 1     & 0     & 1     & 3     & 47.29\% & 61.88\% \\
	1     & 1     & 1     & 0     & 1     & 37.17\% & 58.58\% \\
	1     & 1     & 1     & 0     & 2     & 41.38\% & 59.36\% \\
	1     & 1     & 1     & 0     & 3     & 41.70\% & 59.37\% \\
	1     & 1     & 1     & 1     & 1     & 41.78\% & 60.86\% \\
	1     & 1     & 1     & 1     & 2     & 45.68\% & 61.52\% \\
	1     & 1     & 1     & 1     & 3     & 46.10\% & 61.64\% \\
	\bottomrule
	\label{tab:preprocessing-params}
\end{longtable}%

De tabel leert ons dat een combinatie van een N-Gram tokenizer van maximum 1 woord, het stemmingsalgoritme en de stopwoordfiltering leidt tot het beste resultaat voor beide classificaties. Als we de frequentie weergeven in de woordvector, dan leidt dit meestal tot een daling van het aantal correct geclassificeerde documenten, terwijl de term frequency-inverse document frequency het aantal correct geclassificeerde documenten nog sterker doet dalen. Dit is een opmerkelijke vaststelling die niet lijkt te stroken met de verwachtingen. Mogelijke verklaringen voor dit fenomeen zijn dat de belangrijke termen in teksten van Het Laatste Nieuws mogelijk niet veelvuldig gebruikt worden in artikels. Dit is bijvoorbeeld zo voor artikels waar de eerste minister vermeld wordt. De naam van de eerste minister wordt slechts \'e\'en maal vermeld in het artikel en vervolgens telkens vervangen door synoniemen en verwijzingen zoals \quotes{de eerste minister}, \quotes{de premier}, ... Het zou daarom kunnen dat andere woorden vaker voorkomen dan de eigenlijk typerende woorden voor dit artikel, waardoor een term frequency-inverse document frequency eigenlijk de belangrijkheid van deze termen onderdrukt. Een mogelijke oplossing voor dit probleem zou \textit{synonym detection} (bv. \cite{Zesch2008}, \cite{Weale2009}) zijn. Verschillende andere oplossingen gaan met behulp van bestaande woordenboeken of encyclopedie\"en (bv. \cite{Cheng2013}) op zoek naar termen die gelijkaardig zijn. Deze technieken echter buiten de scope van dit onderzoek en zijn bovendien nog weinig onderzocht voor Nederlandstalige teksten. 

Om het vector space model op te stellen voor de dataset van Het Laatste Nieuws, gaan we dus een N-Gram tokenizer van 1 woord gebruiken en zowel stemming als stopwoordfiltering toepassen. Term frequency-inverse document frequency en frequentie van woorden gebruiken we niet.

Ook voor de dataset van Wikipedia worden de optimale parameters voor preprocessing bepaald op dezelfde manier. De resultaten zijn te vinden in tabel \ref{tab:preprocessing-wiki}

\begin{longtable}{rrrrrrr}
	\caption{Gebruik van verschillende parameters bij preprocessing van de documentencollectie \quotes{Wikipedia} en hun effect op het percentage correct geclassificeerde artikels volgens de \quotes{Naive Bayes} (NB) en \quotes{Naive Bayes Multinomial} (NBM) classifiers in hoofdcategorie\"en.}\\
	\toprule
	\multicolumn{1}{c}{\textbf{tf-idf}} & \multicolumn{1}{c}{\textbf{Stemming}} & \multicolumn{1}{c}{\textbf{Stopwoorden}} & \multicolumn{1}{c}{\textbf{Frequentie}} & \multicolumn{1}{c}{\textbf{Tokenizer}} & \multicolumn{1}{c}{\textbf{NB}} & \multicolumn{1}{c}{\textbf{NBM}} \\
	\midrule
0	& 0	& 0	& 0	& 1	& 34.80\%	& 37.56\% \\
0	& 0	& 0	& 0	& 2	& 33.08\%	& 35.53\% \\
0	& 0	& 0	& 0	& 3	& 32.56\%	& 35.04\% \\
0	& 0	& 0	& 1	& 1	& 28.24\%	& 33.54\% \\
0	& 0	& 0	& 1	& 2	& 28.46\%	& 32.89\% \\
0	& 0	& 0	& 1	& 3	& 28.26\%	& 32.63\% \\
0	& 0	& 1	& 0	& 1	& 36.09\%	& 39.40\% \\
0	& 0	& 1	& 0	& 2	& 33.86\%	& 36.71\% \\
0	& 0	& 1	& 0	& 3	& 33.69\%	& 36.23\% \\
0	& 0	& 1	& 1	& 1	& 30.36\%	& 35.24\% \\
0	& 0	& 1	& 1	& 2	& 29.16\%	& 33.62\% \\
0	& 0	& 1	& 1	& 3	& 28.42\%	& 33.17\% \\
0	& 1	& 0	& 0	& 1	& 35.53\%	& 38.35\% \\
0	& 1	& 0	& 0	& 2	& 33.79\%	& 36.41\% \\
0	& 1	& 0	& 0	& 3	& 33.41\%	& 35.83\% \\
0	& 1	& 0	& 1	& 1	& 27.97\%	& 34.04\% \\
0	& 1	& 0	& 1	& 2	& 28.56\%	& 33.42\% \\
0	& 1	& 0	& 1	& 3	& 28.46\%	& 33.30\% \\
0	& 1	& 1	& 0	& 1	& 36.23\%	& 39.65\% \\
0	& 1	& 1	& 0	& 2	& 34.61\%	& 37.81\% \\
0	& 1	& 1	& 0	& 3	& 34.08\%	& 37.12\% \\
0	& 1	& 1	& 1	& 1	& 29.86\%	& 35.49\% \\
0	& 1	& 1	& 1	& 2	& 29.59\%	& 34.69\% \\
0	& 1	& 1	& 1	& 3	& 29.19\%	& 34.49\% \\
1	& 0	& 0	& 0	& 1	& 37.03\%	& 40.22\% \\
1	& 0	& 0	& 0	& 2	& 35.29\%	& 38.33\% \\
1	& 0	& 0	& 0	& 3	& 34.36\%	& 37.67\% \\
1	& 0	& 0	& 1	& 1	& 37.97\%	& 41.06\% \\
1	& 0	& 0	& 1	& 2	& 37.00\%	& 39.78\% \\
1	& 0	& 0	& 1	& 3	& 35.53\%	& 38.86\% \\
1	& 0	& 1	& 0	& 1	& 37.38\%	& 40.54\% \\
1	& 0	& 1	& 0	& 2	& 35.57\%	& 38.57\% \\
1	& 0	& 1	& 0	& 3	& 34.52\%	& 37.84\% \\
1	& 0	& 1	& 1	& 1	& 38.32\%	& 41.46\% \\
1	& 0	& 1	& 1	& 2	& 36.69\%	& 39.77\% \\
1	& 0	& 1	& 1	& 3	& 35.99\%	& 39.18\% \\
1	& 1	& 0	& 0	& 1	& 37.89\%	& 40.91\% \\
1	& 1	& 0	& 0	& 2	& 36.11\%	& 39.53\% \\
1	& 1	& 0	& 0	& 3	& 35.14\%	& 38.70\% \\
1	& 1	& 0	& 1	& 1	& 38.82\%	& 41.82\% \\
1	& 1	& 0	& 1	& 2	& 37.47\%	& 40.65\% \\
1	& 1	& 0	& 1	& 3	& 36.11\%	& 39.74\% \\
1	& 1	& 1	& 0	& 1	& 37.87\%	& 40.99\% \\
1	& 1	& 1	& 0	& 2	& 36.51\%	& 39.95\% \\
1	& 1	& 1	& 0	& 3	& 35.24\%	& 38.89\% \\
1	& 1	& 1	& 1	& 1	& \textbf{39.27}\%	& \textbf{42.28}\% \\
1	& 1	& 1 & 1	& 2	& 37.64\%	& 40.94\% \\
1	& 1	& 1	& 1	& 3	& 36.26\%	& 40.01\% \\
	\bottomrule
	\label{tab:preprocessing-wiki}
\end{longtable}%

We kunnen opmerken dat over hele lijn, onafhankelijk van de ingestelde parameters, het aantal correct geclassificeerde artikels een stuk lager ligt dan bij Het Laatste Nieuws het geval was. Een mogelijke verklaring ligt in het feit dat de dataset van Wikipedia zo is opgebouwd dat hij per categorie evenveel artikels bevat. Als een classifier dus niet zeker is waar een bepaald artikel terecht moet komen en bijgevolg maar gaat gokken, heeft het systeem maar een kans van 1 op 9 om het artikel alsnog in een juiste categorie onder te brengen. Als we dit vergelijken met Het Laatste Nieuws zien we daar 12 categorie\"en. De verdelingen van de artikels over die categorie\"en is echter niet evenredig. Als een classifier bij deze dataset niet zeker is waar een artikel thuishoort, dan gokt hij op de categorie waar de meeste artikels inzitten. Voor de datset van januari 2008 bevat de categorie nieuws 31,69\% van alle artikels, wat dus een kans van bijna 1 op 3 vertegenwoordigd. Het is dus logisch dat een classifier bij de dataset van Wikipedia een beduidend lagere score haalt voor het aantal correct geclassificeerde artikels.

Een andere verklaring voor dit fenomeen is dat de categorie\"en waarin de artikels van Wikipedia werden ingedeeld veel minder afgebakend zijn dan bij Het Laatste Nieuws. In principe is de structuur van Wikipedia opgebouwd als een hi\"erarchie, weergegeven volgens een acyclische verbonden graaf \cite{Milne2007}. Deze structuur lijkt op een boomstructuur, maar iedere knoop kan verbonden zijn met verschillende andere knopen rondom zich. Er zitten bijgevolg veel overlappen tussen de categorie\"en en er komen zelfs circulaire referenties terug. Als we bijvoorbeeld een categorie \quotes{cultuur} beschouwen vinden we daar artikels over o.a. voeding. Mochten we de graaf voor categorie voeding opstellen krijgen we ongeveer de figuur in \ref{fig:wiki-voeding}.

\begin{figure}[h]
	\caption{Hi\"erarchie binnen Wikipedia van de categorie voeding}
	\label{fig:wiki-voeding}	
	\includegraphics[width=\textwidth]{fig/voeding.png}
\end{figure}

\textbf{TODO: referentie \url{https://tools.wmflabs.org/vcat/catgraphRedirect?wiki=nlwiki&cat=Voeding&format=png&links=wiki}}

Deze figuur toont hoe de categorie voeding niet enkel onder hoofdcategorie cultuur staat, maar ook onder hoofdcategorie\"en \quotes{mens en maatschappij}, \quotes{wetenschap}, \quotes{natuur}, etc. Het is bijgevolg al een goed resultaat dat bepaalde artikels in wezen ingedeeld worden in een correcte structuur. De resultaten van deze dataset vallen dan ook moeilijk te vergelijken met deze van Het Laatste Nieuws, waar wel een boomstructuur van 2 niveaus wordt gebruikt.

\subsection{Feature selection}\label{IG}
Feature selection probeert om een subset van de features te zoeken die relevant zijn voor het doelconcept. Het wordt op verschillende manieren beschreven, maar voor deze use case zijn de criteria de volgende:
\begin{itemize}
\item De nauwkeurigheid van de classificatie daalt niet of nauwelijks,
\item De resulterende klassenverdeling ligt zo dicht mogelijk bij de originele klassenverdeling, gegeven alle features.
\end{itemize}

Om aan deze voorwaarden te voldoen gebruiken we het Information Gain (IG) algoritme. Dit algoritme werkt als volgt: stel dat $P_i$ de globale probabiliteit is van klasse $i$ voorstelt, en $p_{i}(w)$ de probabiliteit van klasse $i$ als het document het woord $w$ bevat. $F(w)$ is dan de fractie van de documenten die het woord $w$ bevatten. Information Gain $I(w)$ is als volgt gedefinieerd voor een gegeven woord  $w$:

\begin{equation}
I(w) = -\sum_{i=1}^{k}P_i\cdot\log{(P_i)} + F(w)\cdot\log(p_i(w)) + (1 - F(w))\cdot\sum_{i=1}^{k}(1-p_i(w))\cdot\log(1-p_i(w))
\end{equation}

Hoe groter de waardes voor IG $I(w)$, hoe groter de discriminerende kracht van het woord $w$. Voor een corpus met $n$ documenten en $d$ woorden is de complexiteit van IG $O(n \cdot d \cdot k)$ \cite{Forman2003}.

We testen de implementatie van IG uit  in combinatie met het Ranker-algoritme in WEKA, op het woordenboek met de gekozen opties bij preprocessing voor de dataset van Het Laatste Nieuws. Dit levert de resultaten in tabel \ref{tab:feature-selection}.

\begin{table}[htbp]
	\centering
	\caption{Vergelijking voor en na feature selection met de Information Gain-methode voor dataset Het Laatste Nieuws, januari 2008}
	\begin{tabular}{rrrrrrr}
		\toprule
		\multicolumn{3}{c}{\textbf{Origineel}} & \multicolumn{1}{c|}{\textbf{}} & \multicolumn{3}{c}{\textbf{Information Gain}} \\
		\midrule
		\textbf{NB} & \textbf{NBM} & \textbf{woorden}    &   \multicolumn{1}{c|}{}   & \textbf{NB} & \textbf{NBM} & \textbf{woorden} \\
		71.91\% & 76.37\% & 1000  &   \multicolumn{1}{c|}{}    & 72.01\% & 76.51\% & 893 \\
		\bottomrule
	\end{tabular}%
	\label{tab:feature-selection}%
\end{table}%

We merken een sterke daling in het aantal woorden (10,70\%), maar tergelijkertijd een lichte stijging in het aantal juist geclassificeerde documenten (voor de "Naive Bayes$"$ classifier 0,10\%, voor de "Naive Bayes Multinomial" classifier 0,14\%).  We kunnen hier dus besluiten dat IG een algoritme is dat goede resultaten geeft voor tekstuele data. De zogenaamde ruis wordt weggenomen uit de woordvector en de belangrijke woorden werden behouden.

Als we diezelfde IG toepassen op de dataset van Wikipedia krijgen we de resultaten in tabel \ref{tab:feature-selection-wiki}.

\begin{table}[htbp]
	\centering
	\caption{Vergelijking voor en na feature selection met de Information Gain-methode voor dataset Wikipedia}
	\begin{tabular}{rrrrrrr}
		\toprule
		\multicolumn{3}{c}{\textbf{Origineel}} & \multicolumn{1}{c|}{\textbf{}} & \multicolumn{3}{c}{\textbf{Information Gain}} \\
		\midrule
		\textbf{NB} & \textbf{NBM} & \textbf{woorden}    &   \multicolumn{1}{c|}{}   & \textbf{NB} & \textbf{NBM} & \textbf{woorden} \\
		39.26\% & 42.27\% & 1000  &   \multicolumn{1}{c|}{}    & 34.93\% & 44.92\% & 600 \\
		\bottomrule
	\end{tabular}%
	\label{tab:feature-selection-wiki}%
\end{table}%

Bij deze dataset merken we een heel sterke daling in het aantal woorden op (40 \%). Dit duidt op het feit dat veel woorden gevonden worden die weinig bijdragen tot een categorie. Ook dit kan herleid worden naar het feit dat de classifier er moeite mee heeft een eenduidige definitie op te stellen van een categorie. Ondanks de sterke daling in woorden zien we een stijging in het aantal correct geclassificeerde artikels voor de NBM-methode (2,65\%). Voor de na√Øeve Bayesiaanse methode merken we wel een daling op van 4,33 \%. Ondanks deze daling is de bewijst het IG algoritme zijn nut door het aantal woorden sterk te doen dalen en tegelijkertijd een stijging in het aantal correct geclassificeerde artikels te bekomen voor de NBM-methode.

%TODO: Verbeterd tot hier
\subsection{Classificatie in hoofdcategorie\"en}
We starten met het classificeren in hoofdcategorie\"en. De dataset van januari 2008 bevat 11 hoofdcategorie\"en: Auto, Bizar, Geld, iHLN, Muziek, Nieuws, Reizen, Showbizz, Sport, Wetenschap en You. De verdeling van de artikels over deze hoofdcategorie\"en kan gevonden worden in tabel \ref{tab:hln-2008-01-cat}.

Om de best mogelijke classificatie te vinden, wordt een testbank opgesteld waarin een selectie van algoritmes voor classificatie in WEKA (46) opgenomen zijn. Enkele classifiers werden niet opgenomen omdat ze o.a. manuele input verwachten. Omdat dit een geautomatiseerde toepassing is, is het niet mogelijk om manueel zaken aan te passen. De gekozen classifiers worden met de standaardopties uitgevoerd op de data van Het Laatste nieuws uit januari 2008, die vooraf door de preprocessing-fase gegaan is. De classifiers worden vervolgens gerangschikt volgens het percentage correct geclassificeerde artikels, gebruik makend van een 10-fold cross-validation (zie \ref{10-fold-cv}). De resultaten zijn te vinden in \textbf{bijlage %TODO
}.

Alles testen werden uitgevoerd op een desktop computer met een Intel Core i5-2500k (3,3Ghz) CPU en 8GB RAM. 

\subsubsection{Na\"ieve baseline classifier}\label{naieve-classifier}
Het aantal woorden die opgenomen wordt in de preprocessing van de documentencollectie wordt gevarieerd van 100 t.e.m. 20000 attributen met telkens een increment van 100 attributen. Nadien wordt de Information Gain methode (zie \ref{IG}) toegepast om eventuele ruis te verwijderen. Snel - na 2000 attributen - merken we echter dat enkele methodes het heel erg slecht doen. Een eerste zeer na\"ieve classifier waarmee vergeleken wordt is een classifier die alle artikels toekent aan de categorie met het meeste artikels (in dit geval nieuws). Deze na\"ieve implementatie zal in principe 31,69\% correct classificeren. In WEKA wordt deze ge\"implementeerd onder de naam ZeroR. Dit algoritme is ook opgenomen in lijst van classifiers. 

\begin{table}[htbp]
	\centering
	\caption{Classifiers die het slechter of even goed doen als de na\"ieve baseline classifier (ZeroR) bij classificatie van artikels in hoofdcategorie\"en na bijhouden van 2000 woorden in de woordvector.}
	\begin{tabular}{rrr}
		\toprule
		& 1000  & 2000 \\
		\midrule
		CVParameterSelection & \multicolumn{1}{c}{31.70\%} & \multicolumn{1}{c}{31.70\%} \\
		Grading & \multicolumn{1}{c}{31.70\%} & \multicolumn{1}{c}{31.70\%} \\
		MultiScheme & \multicolumn{1}{c}{31.70\%} & \multicolumn{1}{c}{31.70\%} \\
		Stacking & \multicolumn{1}{c}{31.70\%} & \multicolumn{1}{c}{31.70\%} \\
		StackingC & \multicolumn{1}{c}{31.70\%} & \multicolumn{1}{c}{31.70\%} \\
		Vote  & \multicolumn{1}{c}{31.70\%} & \multicolumn{1}{c}{31.70\%} \\
		ZeroR & \multicolumn{1}{c}{31.70\%} & \multicolumn{1}{c}{31.70\%} \\
		VFI   & \multicolumn{1}{c}{13.02\%} & \multicolumn{1}{c}{8.93\%} \\
		\bottomrule
	\end{tabular}%
	\label{tab:naive-classifiers}%
\end{table}

Als we ZeroR als baseline classifier beschouwen om te bepalen wat een slechte classifier is, dan vinden we enkele methodes die deze baseline niet halen en die het m.a.w. slechter of net even goed doen als ZeroR bij 2000 woorden. De zwakke classifiers zijn VFI (Voting Feature Intervals), Vote, Stacking, StackingC, MultiScheme, Grading, CVParameterSelection en ClassificationViaClustering. Als we enkele van deze algoritmes van dichterbij gaan bekijken, zien we dat o.a. Vote, VFI, Stacking, StackingC, MultiScheme en Grading enkele extra parameters verwachten waarin (enkele) andere classifiers worden opgegeven. Die methodes gaan dus andere classifiers combineren en worden verder besproken in \ref{combineren-classifiers}, waar de voordelen van het combineren van classifiers wordt onderzocht. 

CVParameterSelection is een classifier die een wrapper is rond een andere classifier. Deze functie is handig om te bepalen welke parameters van een bepaalde functie optimaal presteren. Omdat deze functie veel rekenkracht vraagt en op zich geen echte classificatie uitvoert, wordt deze hier niet verder behandeld. 

Het is voor het verder onderzoek naar de beste classifiers voor deze dataset niet nuttig dat de vermelde classifiers verder individueel onderzocht worden. Om rekenkracht te sparen worden deze dan ook niet meer opgenomen in de resultaten na 1000 woorden.

Enkele algoritmes vallen hier ook uit de boot omdat ze \textit{out-of-memory} gaan bij 1000 of minder attributen. Ze leveren overigens ook een slechtere classificatie dan de op dat moment beste classifiers (zie tabel \ref{tab:out-of-memory}). Ook deze kunnen niet verder onderzocht worden.

\begin{table}[htbp]
	\centering
	\caption{Classifiers die out-of-memory gaan bij classificatie van artikels in hoofdcategorie\"en na bijhouden van 900 woorden in de woordvector.}
	\begin{tabular}{rrr}
		\toprule
		& 900 \\
		\midrule
		MultiClassClassifier & \multicolumn{1}{c}{69.11\%} \\
		LogitBoost & \multicolumn{1}{c}{73.27\%} \\
		ClassificationViaRegression & \multicolumn{1}{c}{73.16\%} \\
		SimpleCart & \multicolumn{1}{c}{71.25\%} \\
		OrdinalClassClassifier  & \multicolumn{1}{c}{64.81\%} \\
		\bottomrule
	\end{tabular}%
	\label{tab:out-of-memory}%
\end{table}


\subsubsection{One Rule}
Enkele methodes doen het net iets beter dan de na\"ieve ZeroR baseline, maar leveren geen verbetering op naar mate het aantal woorden toeneemt. We vergelijken de methodes nu met de \textit{OneR} methode. OneR is een simpel classificatie algoritme dat \'e\'en regel voor elk attribuut in de data genereert. Het selecteert dan de regel met de kleinste totale fout als zijn \quotes{one rule}. Om een regel op te stellen voor een attribuut, construereert OneR een frequentietabel voor elk attribuut tegenover het doelobject.

\begin{table}[htbp]
	\centering
	\caption{Classifiers die het slechter of even goed doen als de OneR methode bij classificatie van artikels in hoofdcategorie\"en na bijhouden van 2000 woorden in de woordvector.}
	\begin{tabular}{rrr}
		\toprule
		& \multicolumn{1}{c}{1000} & 2000 \\
		\midrule
		ConjunctiveRule & \multicolumn{1}{c}{49.64\%} & \multicolumn{1}{c}{49.60\%} \\
		AdaBoostM1 & \multicolumn{1}{c}{49.61\%} & \multicolumn{1}{c}{49.61\%} \\
		MultiBoostAB & \multicolumn{1}{c}{49.61\%} & \multicolumn{1}{c}{49.61\%} \\
		OneR  & \multicolumn{1}{c}{49.61\%} & \multicolumn{1}{c}{49.61\%} \\
		DecisionStump & \multicolumn{1}{c}{49.61\%} & \multicolumn{1}{c}{49.61\%} \\
		ClassificationViaClustering & \multicolumn{1}{c}{39.56\%} & \multicolumn{1}{c}{39.38\%} \\
		\bottomrule
	\end{tabular}%
	
	\label{tab:addlabel}%
\end{table}%
 

De classifiers die het slechter of even goed doen als de OneR methode zijn: ConjunctiveRule, AdaBoostM1, MultiBoostAB, DecisionStump en ClassificationViaClustering. Opnieuw zijn enkele van deze classifiers wrappers rond anderen. AdaBoostM1 en MultiBoostAB zijn boosting algoritmes (zie \ref{boosting}) waarvan de default classifier DecisionStump is. Boosting kan een interessant gegeven zijn wanneer bepaalde klasses weinig gekozen worden door het classificatiealgoritme.

De bovenvermelde algoritmes worden niet verder getest met meer dan 2000 woorden, omdat ze geen goede resultaten opleveren.  	

\subsubsection{Naive Bayes}
Een volgend algoritme die als baseline gebruikt wordt is de Naive Bayes classifier bij 3000 woorden. De na\"ieve Bayesiaanse methode werkt volgens de regel van Bayes \cite{Zhang2004}. Deze regel geeft een kans dat een bepaald voorbeeld $E = (x_1, x_2, ..., x_3)$ aan klasse $c$ toebehoord als volgt:

\begin{equation}
p(c|E) = \frac{p(E|c)p(c)}{p(E)}
\end{equation}

$E$ wordt geclassificeerd als klasse $C$ = + als

\begin{equation}
f_b(E) = \frac{p(C = +|E)}{p(C = -|E)} \geq 1,
\end{equation}

Daarbij wordt $f_b(E)$ de Bayesiaanse classifier genoemd. Veronderstel dat alle attributen onafhankelijk zijn van de gegeven klassevariable: 

\begin{equation}
p(E|c) = p(x_1,x_2,...,x_n|c) = \prod_{i=1}^{n}p(x_i|c)
\end{equation}

Dan is de resulterende classifier:

\begin{equation}\label{bayesiaanse-classifier}
f_nb(E) = \frac{p(C = +)}{p(C = -)}\prod_{i=1}^{n}\frac{p(x_i|C = +)}{p(x_i|C = -)}
\end{equation}

Formule \ref{bayesiaanse-classifier} geeft dan de Bayesiaanse classifier. Dit is de simpelste vorm van het Bayesiaans netwerk, waar alle attributen onafhankelijk gegeven zijn van de klassevariabele. 



Enkele van de beschouwde methodes in deze stap zijn ook boosting- of combinatieclassifiers. LogitBoost en RacedIncrementalLogitBoost zijn beiden boosting classifiers (zie \ref{boosting}) en ClassificationViaRegression is een classifier die gebruikt wordt om bij stacking of grading (zie \ref{combineren-classifiers}) methodes de verschillende classifiers samen te voegen. Met hun default implementatie leveren ze echter geen verbetering op tegenover de 

Enkele methodes vallen af omdat ze te veel geheugen gebruiken en dus niet meer werken voor 5000 attributen. Dit zijn MulticlassClassifier, J48graft en SimpleCart. Hun maximaal behaalde percentage voor de documentencollectie, bij hun respectievelijk hoogst mogelijke aantal attributen kan terug gevonden worden in tabel \ref{tab:naive-bayes-out-of-memory}.

\textbf{TODO}



\subsection{Combineren van classifiers voor hoofdcategorie\"en}
\label{combineren-classifiers}
\subsubsection{Voting}
Het is mogelijk dat we door verschillende goede individuele classifiers te combineren, een betere algemene classificatie bekomen. Een algoritme dat precies daarvoor gebruikt wordt is "Vote"  \cite{Kittler1998}. Dit is een algoritme dat zelfstandig geen classificatie uitvoert (zie ook resultaten individuele classifiers, \ref{naieve-classifier}). Het neemt een aantal zelf gekozen classifiers samen en laat die stemmen om te bepalen tot welke klasse een bepaalt document behoort. Om er zeker van te zijn of dit een effectieve verbetering oplevert passen we het "Vote$"$-algoritme toe met de top 4 beste algoritmes voor classificatie in hoofdcategorie\"en: DMNBText, MultinomialNaiveBayes, SMO en ComplementNaiveBayes. 

\begin{table}[htbp]
	\centering
	\caption{Aantal correct geclassificeerde artikels bij gebruik van het Vote-algoritme met classifiers DMNBText, MultinomialNaiveBayes, SMO en ComplementNaiveBayes voor dataset Het Laatste Nieuws (januari 2008) met verschillende combinatiemethodes.}
	\begin{tabular}{rr}
		\toprule
		Combinatiemethode & Correct geclassificeerd \\
		\midrule
		Average of Probabilities & 89,65\% \\
		Product of Probabilities & 86,45\% \\
		Majority Voting & 89,92\% \\
		Minimum Probability & 86,45\% \\
		Maximum Probability & 86,56\% \\
		\bottomrule
	\end{tabular}%
	\label{tab:vote}%
\end{table}%

Het Vote-algoritme laat toe om met verschillende methodes de originele classificaties te combineren. Deze worden voorgesteld in tabel \ref{tab:vote}, samen met het percentage correct geclassificeerde artikels voor de preprocessed dataset van januari 2008.

De methode "Majority Voting" geeft een licht beter resultaat dan de beste individuele classifier (DMNBText; 89,86\% correct geclassificeerd). Het verschil valt echter te verwaarlozen (0,06\%). 

\subsubsection{Stacking}\label{stacking}
Een andere methode om mogelijk een beter classificatie te bekomen is het "Stacking$"$-algoritme. Meestal presteert dit beter dan voting \cite{Sigletos2005}. \textit{\textit{Stacking\textit{}}} wordt gezien als een generalisatie van het "Vote$"$-algoritme. Het combineert de voorspellingen van verschillende andere classifiers en past een zelfgekozen combinatie-algoritme toe met de voorspellingen van de andere algoritmes als extra input. 

Het combineert meerdere classifiers die gegenereerd zijn door verschillende lerende algoritmes $L_1$,...,$L_n$ op een dataset S toe te passen. De dataset bestaat uit elementen in de vorm van $s_i = (x_i, y_i)$, welke in dit geval de paren van de feature vectoren ($x_i$) met hun classificatie ($y_i$) voorstellen. In de eerste fase worden base-level classifiers $C_1$,...,$C_n$ gegenereerd, waar $C_i = L_{i}(S)$. In de tweede fase wordt een meta-level classifier geleerd om de output van de base-level classifiers te combineren.

Ook hier passen we stacking toe met de vier beste individuele classifiers. We krijgen de resultaten in tabel \ref{tab:stacking}

\textbf{TODO: resultaten}
LinearRegression 89,98\%

\subsubsection{Grading}\label{grading}
Een andere methode om mogelijk een beter classificatie te bekomen is met het "Grading$"$-algoritme \cite{SEEWALD}. \textit{Grading} maakt gebruikt van \textit{graded} voorspellingen voor een trainingsset van meta classifiers die leren voorspellen wanneer de basisclassifier correct is. De trainingsset van deze meta classifiers is geconstrueerd gebruik makend van graded voorspellingen van de corresponderende basisclassifier als de nieuwe klasselabels voor de originele attributen. 

\textbf{TODO: resultaten}
\begin{table}[htbp]
	\centering
	\caption{Aantal correct geclassificeerde artikels bij gebruik van het Grading-algoritme met classifiers DMNBText, MultinomialNaiveBayes, SMO en ComplementNaiveBayes voor dataset Het Laatste Nieuws (januari 2008)}
	\begin{tabular}{rr}
		\toprule
		Metaclassifier & Correct geclassificeerd \\
		\midrule
		ZeroR & 89,70\% \\
		\bottomrule
	\end{tabular}%
	\label{tab:grading}%
\end{table}%

\subsection{Ensemble learning voor hoofdcategorie\"en}
\subsubsection{Boosting}\label{boosting}
Boosting is een algemene methode om de performantie van lerende algoritmes te verbeteren. Het kan gebruikt worden om de fout van een zwak lerend algoritme dat consistent classifiers genereert die slechts enigszins beter zijn dan random gokken, significant te verlagen \cite{Freund1996}.


\textbf{TODO}

\subsubsection{Bagging}
\textbf{TODO}
 

\subsection{Classificatie in subcategorie\"en}
Het is mogelijk dat een classificatie in subcategorie\"en een ander algoritme vereist dan het classificeren in hoofdcategorie\"en. Daarom testen we ook voor de subcategorie\"en welke classifier de beste resultaten geeft. 

%TODO

\subsection{Combineren van classifiers voor subcategorie\"en}
\subsubsection{Voting}
Om tot een betere classificatie te komen voor subcategorie\"en, worden de beste 8 classifiers gebruikt bij hun respectievelijke optimale aantal woorden, gebruik makend van \quotes{Majority Voting}. De resultaten worden samengevoegd in tabel \ref{tab:vote-sub}.

%TODO


\subsubsection{Stacking}
\subsubsection{Grading}
\subsection{Ensemble learning voor subcategorie\"en}
\subsubsection{Bagging}
\subsubsection{Boosting}

\section{Clustering}
Clustering is een methode die gebruikt wordt om documenten met gelijkaardige inhoud in eenzelfde cluster te groeperen \cite{Hotho2005}. Het wordt naast classificatie toegepast om gelijkaardige documenten te vinden over de grenzen van de classificatie heen (documenten die aan verschillende categorie\"en toebehoren, maar bv. over dezelfde persoon handelen). Ook zal clustering ervoor zorgen dat documenten binnen dezelfde categorie toch nog verder opgedeeld kunnen worden in bepaalde topics. Op deze manier kan meer informatie uit de teksten gehaald worden en die informatie kan op zijn beurt gebruikt worden door het aanbevelingssysteem. 

Elke cluster bevat een aantal documenten. Het is belangrijk dat documenten in een bepaalde cluster gelijkaardig zijn aan elkaar, maar verschillen met documenten in andere clusters. Cluster methodes groeperen documenten op basis van hun distributie in een bepaalde documentenruimte. We krijgen zo bijvoorbeeld een $n$-dimensionele ruimte als we het vector space model zouden gebruiken.

Clustering wordt berekend op basis van attributen van de tekst. Nochtans is het concept \quotes{cluster} een subjectief begrip. Wat de ene persoon ziet als een goede en relevante cluster, kan door iemand anders niet als een goede cluster aanzien worden. Clustering vraagt bijgevolg om een duidelijk evaluatiecriterium (zie \ref{evaluatie-clustering}), zodat we objectief kunnen vaststellen wat een goede cluster is.

\subsection{K-Means Clustering algoritme}
Een populair clustering algoritme is het K-Means clustering algoritme. Voor een gegeven dataset $X = {x_1,...x_n}, x_n \in R^d$ wordt geprobeerd om deze data in $M$ verschillende subsets (clusters) $C_1,...,C_M$ te partitioneren, zodat de clustervoorwaarde voldaan is \cite{Likas2011}. Een gedetailleerde werking van dit algoritme kan gevonden worden in 

Het K-means algoritme wordt meestal iteratief uitgevoerd voor elke k. Om een optimale clustering te bekomen wordt gebruik gemaakt van Natural Language Processing (zie \ref{nlp}). Het nadeel van elke $k$ iteratief uit te proberen is de uitvoeringstijd bij grote documentencollecties. Om deze te beperken worden een aantal verschillende methodes uitgeprobeerd met als doel de uitvoeringstijd te verlagen (zie \ref{spark}, \ref{mahout}).

Een ander probleem met dit algoritme is dat bij de start van het algoritme de cluster centra steeds random ge\"initialiseerd worden. Daarom moet het algoritme herhaaldelijk de verschillende clusters opnieuw verdelen om tot een optimale oplossing te komen. Om sneller deze optimale waarden te kunnen bepalen worden voorstellen uitgewerkt in \ref{mahout}

\subsection{Evaluatiecriteria voor clustering}\label{evaluatie-clustering}
Er bestaan twee manieren om resultaten van clustering te evalueren. Enerzijds kunnen statistische functies gebruikt worden om de eigenschappen van de clustering te beschrijven. Anderzijds kan classificatie gebruikt worden als \quotes{gold standard}, waarbij de clustering vergeleken wordt met de gegeven classificatie. Omdat het doel van de clustering in deze toepassing echter niet is om classificatie door te voeren, zijn we aangewezen op de statistische functies. Om te bepalen wat een goede cluster is, moeten we voor het $k$-means algoritme bepalen hoeveel clusters $k$ een dataset bevat. Hiervoor worden enkele criteria besproken.

\subsubsection{Vuistregel}
De vuistregel \cite{Science2013} is een simpele regel, die toegepast kan worden op elke gegeven dataset. De uitkomst van deze regel wordt enkel als een indicatie beschouwd en kan zeker niet gebruikt worden voor het bepalen van het effectieve aantal clusters. 
\begin{equation}
k = \sqrt{\frac{n}{2}}
\end{equation}

\subsubsection{Elbow method}
De \quotes{elbow method} \cite{Science2013} is een visuele methode. Het idee is om alle waarden van $k$ te itereren en telkens een kostenfunctie te plotten op een grafiek. Bij een bepaalde waarde van $k$ daalt de kost sterk, waarna het een plateau bereikt als $k$ verder ge\"itereerd wordt. 

Het is echter niet altijd gemakkelijk om deze \quotes{elbow} te vinden, zeker niet indien de documentencollectie groot wordt. Ze vereist enerzijds dat we alle clusters aflopen, maar vereist anderzijds ook dat een functie kan gemaakt worden die de optimale $k$ kan vinden. Die functie werd gevonden in \cite{Tibshirani2001} met de zogenaamde \quotes{Gap statistic}. Deze methode is echter niet bruikbaar voor grote documentencollecties omdat ze vereist dat eerst alle clusters worden gevonden, voor de optimale $k$ kan gevonden worden. 

\subsubsection{Inter- versus intra-cluster afstand}
Dit criterium werd voorgesteld in \cite{Ray} en probeert om de som van de kwadratische afstanden van alle punten tot hun cluster centrum te minimaliseren. Hiervoor moeten we de afstanden van elk document tot cluster centrum berekenen. Deze worden gegeven door:

\begin{equation}
intra = \frac{1}{N}\sum_{i=1}^{K}\sum_{x\in{C_i}}^{} \vert\vert \mathbf{x-z_i} \vert\vert ^2 
\end{equation}
waarbij $N$ het aantal documenten is in de documentencollectie, $K$ het aantal clusters en $z_i$ het cluster centrum van cluster $C_i$.

Vervolgens berekenen we de inter-cluster afstand. We berekenen dit als het minimum van de afstanden tussen de cluster centra:

\begin{equation}
intra = min(\vert\vert \mathbf{z_i-z_j}\vert\vert^2) ; i = 1,2,...,K-1 ; j = i+1, ..., K
\end{equation}

Van deze reeks hebben we enkel het minimum nodig, omdat we deze waarde willen maximaliseren. De andere clusters liggen sowieso verder van elkaar.

We combineren beide waarden vervolgens tot de \quotes{validity measure}:

\begin{equation}
validity = \frac{intra}{inter}
\end{equation}

Omdat we de intra-cluster afstand steeds willen minimaliseren en de inter-cluster afstand willen maximaliseren, moet de validity measure telkens geminimaliseerd worden (minimum van de teller delen door maximum in de noemer zorgt voor minimalisatie van de validity measure). Omdat deze validity measure steeds daalt stellen we een bepaalde \textit{treshold} in, zodat de clusters niet te specifiek worden (en dus heel weinig relevante buren zouden overhouden). Deze waarde wordt experimenteel bepaald in tabel \ref{tab:clusters-validity} voor een kleine dataset (20 documenten). Daarbij gaan we het iteratieve k-means clustering algoritme toepassen op de dataset en bij elke uitvoer de validity-measure nakijken. De teksten zijn gekozen uit de dataset van Het Laatste Nieuws en zijn zo geselecteerd dat er artikels van 5 totaal verschillende topics aanwezig zijn (de zwangerschap van Angelina Jolie, Osama Bin Laden, transfers van voetbalploeg Anderlecht, Maddie McCan en de Amerikaanse presidentsverkiezingen).

Het cluster algoritme dat gebruikt wordt past k-means clustering toe met een feature vector van 1000 features. Het algoritme krijgt 100 iteraties om de optimale cluster centra te bepalen voor k-means en dit voor elke k. Er worden drie afstandsfuncties (\textit{similarity measures}) gebruikt \cite{Huang2008}: Euclidean distance, cosine distance en Jaccard similarity. Voor elke iteratie wordt a.d.h.v. de vooraf opgesteld lijst van topics vergeleken hoe de clustering presteert. 

\begin{table}[htbp]
  \centering
  \caption{Berekenen van de validity-measure voor verschillende afstandsfuncties (euclidean distance, cosine distance en jaccard similarity) bij k-means algoritme (1000 features, 100 iterations)}
    \begin{tabular}{rrrr}
    \toprule
    \multicolumn{1}{c}{k} & euclidean & cosine & jaccard \\
    \midrule
    1     & 1.80  & 1.80   & 1.80 \\
    2     & 0.48   & 1.13  & 0.81 \\
    3     & 0.20  & 0.63   & 0.72 \\
    4     & 0.17   & 0.61    & 0.63 \\
    5     & 0.17  & 0.51  & 0.59 \\
    6     & 0.17  & 0.42   & 1.79 \\
    7     & \textbf{0.16}  & 0.37  & 0.53 \\
    8     & 0.12  & \textbf{0.33}  & 1.80 \\
    9     & 0.12  & 0.29  & 1.80 \\
    10    & 0.12  & 0.21  & 1.80 \\
    11    & 0.10  & 0.22 & 1.80 \\
    12    & 0.09  & 0.19  & 1.80 \\
    13    & 0.07  & 0.15  & 1.80 \\
    14    & 0.05  & 0.13   & 1.80 \\
    15    & 0.04  & 0.09  & 1.80 \\
    \bottomrule
    \end{tabular}%
  \label{tab:clusters-validity}%
\end{table}%

De subjectief bepaalde optimale validity measures zijn in het vet aangeduid. Dit zijn de validity measures waarbij de topics het best lijken op de vooropgestelde topics (of delen daarvan). Voor de Jaccard similarity werd geen enkele clustering gevonden waarbij dicht van de te bekomen clustering gekomen wordt. Bovendien komen we met de cosine similarity het dichtst bij de vooraf opgestelde clustering. Geen enkel algoritme slaagt er echter in de vooropgestelde 5 topics te detecteren. Omdat de cosine distance het best presteert werken we verder met deze afstandsfunctie. 

\subsection{Named entity recognition}
Veel van de clusters die geconstrueerd worden handelen over een bepaald onderwerp. In de recente actualiteit zouden we bijvoorbeeld een cluster rond ISIS kunnen vinden die handelt over terrorisme. Die bepaalde cluster kan ge\"identificeerd worden door zogenaamde \textit{named entities}.

Dit idee werd ge\"introduceerd door o.a. \cite{Montalvo2015}, waar een systeem gebouwd werd dat artikels clusterde uit twee verschillende talen door gebruik te maken van \textit{natural language processing} (NLP) en named entity recognition. Zij gaan uit van het idee dat een artikel (en bij uitbreiding een tekst) geschreven is met zes vragen in het achterhoofd: wat, wie, wanneer, waar, waarom en hoe. Een belangrijk deel van deze vragen kan beantwoord worden met een named entity (NE). Antwoorden op \textit{waar} geven bijvoorbeeld namen van locaties, terwijl wanneer een datum of tijd teruggeeft. Als ervan uit gegaan wordt dat:
\begin{itemize}
\item NEs over verschillende, gelijkaardige documenten behouden blijven omdat het moeilijk is om een naam te parafraseren,
\item de frequentie van NEs in zinnen de belangrijkheid benadrukt van gebeurtenissen waar ze mee geassocieerd zijn,
\item twee verschillende verhalen met dezelfde NEs meer dan waarschijnlijk aan dezelfde topic toebehoren,
\end{itemize} dan kunnen we een model maken dat specifieke clusters rond deze NEs kan bouwen.

Omdat het gemakkelijker ligt om topics te vinden rond personen (PERS), organisaties (ORG), locaties (LOC) en andere (MISC) worden enkel deze NEs gebruikt voor het model. Andere categorie\"en zoals tijd, datum of cijfers worden niet als belangrijk gezien omdat ze een hoge kans geven op extra ruis als ze niet voorzichtig gebruikt worden. Er kunnen bijvoorbeeld twee belangrijke gebeurtenissen in de geschiedenis gebeurd zijn op hetzelfde tijdstip, maar de gebeurtenissen kunnen totaal verschillen. Zeker bij datasets zoals die van Het Laatste Nieuws, waar veel topics op dezelfde dag worden behandeld, moet daarvoor opgelet worden. 

Sommige auteurs veronderstellen echter dat documenten clusteren enkel rond NEs schadelijk kan zijn voor clustering (o.a. \cite{Friburger2002}, \cite{Gliozzo2005}). Ze veronderstellen dat NEs alleen niet genoeg informatie geven over een tekst om een dataset correct te clusteren. Daarom wordt in dit geval een model gebruikt waarin NEs een belangrijkere rol krijgen dan andere features. Nadat de documentencollectie het stadium van preprocessing doorlopen heeft en de feature vector wordt opgebouwd, krijgen NEs een groter gewicht toegekend dan de andere features. 

\subsubsection{Implementatie van NLP}
Om gebruik te maken van NLP en zodoende de NEs uit een tekst te halen, wordt gebruik gemaakt van de Stanford Named Entity Recognizer \cite{Finkel2005} met een Nederlandse classifier. Binnen dit systeem worden NEs getagd als volgt: PER (personen), ORG (organisaties), LOC (locaties) en MISC (andere). Door de term frequency van deze NEs te verhogen worden ze belangrijker voor een tekst en wordt geprobeerd om een betere clustering te verkrijgen. 

Omdat deze classifier enkel voor het Nederlands werkt, zijn er voor deze classifier geen grote evaluatiesets beschikbaar. Daarom wordt een set van 1930 woorden zelf getagd waarop ge\"evalueerd wordt. De globale precisie van deze classifier was van 97,31\% (1878/1930 objecten werden correct getagd). De individuele scores kunnen gevonden worden in tabel \ref{tab:nlp-classifier}. 

\begin{table}[htbp]
  \caption{Precisie van een Nederlandse classifier voor Stanford NLP}
  \centering
    \begin{tabular}{rrrrrr}
    \toprule
          & Aantal tags & antwoorden & Precisie & Recall & F-score \\
    \midrule
    LOC   & 26    & 22    & 0.95  & 0.81  & 0.88 \\
    PER   & 30    & 28    & 0.75  & 0.7   & 0.72 \\
    MISC  & 22    & 18    & 0.67  & 0.55  & 0.6 \\
    ORG   & 7     & 17    & 0.35  & 0.85  & 0.5 \\
    O     & 1845  & 1793  & 0.97  &       &  \\
    \bottomrule
    \end{tabular}%
  \label{tab:nlp-classifier}%
\end{table}%

De NLP wordt ge\"integreerd tijdens de preprocessing. Tussen de stap van tokenization en stopwoordfiltering komt een extra stap die de NEs uit een tekst een hogere frequentie geeft tegenover de andere elementen. Op die manier wordt een woord belangrijker voor een tekst en wordt verwacht dat hij vaker als discriminerende factor zal dienen bij het clusteren. 

\subsubsection{Resultaten voor kleine datasets}
Er wordt terug gegrepen naar de dataset met 20 documenten en 5 topics uit de vorige sectie. We passen opnieuw dezelfde clustering toe, maar gaan de named entities deze keer een groter gewicht geven. We vinden volgende clustering met een validity measure van 0.31:
\\

\begin{lstlisting}
Cluster 0
  Document: 0, Title: Pakistan niet op zoek naar bin Laden
  Document: 5, Title: Hoe Osama bin Laden de Amerikanen ontglipte in 2001
  Document: 8, Title: Zoon Bin Laden wil in Groot Brittannie wonen
  Document: 13, Title: Bin Laden junior wil dat pa ermee kapt
Cluster 1
  Document: 1, Title: Anderlecht verhuurt Cyril Thereau aan Charleroi
  Document: 12, Title: Anderlecht huurt Yakovenko
  Document: 16, Title: Anderlecht strikt Guillaume Gillet
  Document: 18, Title: Luigi Pieroni naar Anderlecht
Cluster 2
  Document: 2, Title: McCanns onderhandelen over verfilming Maddie
  Document: 3, Title: McCanns verdenken politie onderzoek te vertragen
  Document: 14, Title: McCanns verspreiden foto van mogelijke ontvoerder
  Document: 19, Title: Foto van kidnapper is afleidingstruc van McCanns
Cluster 3
  Document: 4, Title: Angelina is zwanger geen twijfel mogelijk
  Document: 6, Title: Angelina zwanger van tweeling
  Document: 10, Title: Jurk Angelina wakkert geruchten over zwangerschap aan
  Document: 11, Title: Angelina Jolie en Brad Pitt oefenen voor tweede kind
Cluster 4
  Document: 7, Title: Analyse waarom South Carolina Obama zuur kan opbreken
  Document: 9, Title: Clinton en Obama strijden in South Carolina
  Document: 15, Title: Clinton en McCain op kop in race naar Witte Huis
  Document: 17, Title: Analyse Wie is John McCain
\end{lstlisting}

Door het gebruik van NEs wordt de vooropgestelde clustering perfect gevonden in de kleine dataset. We testen vervolgens deze clustering op een grotere dataset in de volgende paragraaf.

\subsubsection{Resultaten voor grote datasets}
Het clusteralgoritme met NLP wordt toegepast op een dataset met 10121 documenten (Het Laatste Nieuws, januari 2008). De validity measure wordt op 0,32 gezet en het algoritme wordt uitgevoerd met iteratieve k (1000 features, 100 iteraties om cluster centra te bepalen). Van bij het begin kunnen we opmerken dat dit algoritme niet zal kunnen gebruikt worden voor grote hoeveelheden data. De totale tijd die nodig is om clusters te maken stijgt bijna exponentieel en neemt al snel een uur in beslag om 25 clusters te vinden (zie afbeelding \ref{fig:clustering-10000-exp}). Op dat moment krijgt de clustering een validity measure van 0,93 en zijn de clusters dus nog niet gevormd.

\begin{figure}[h]
	\caption{Aantal clusters in het iteratieve k-means algoritme in relatie tot het aantal clusters voor een dataset met 10121 documenten}
	\label{fig:clustering-10000-exp}	
	\includegraphics[width=\textwidth]{fig/clustering-10000-exp.png}
\end{figure}

Om de totale tijd die het algoritme nodig heeft te doen dalen worden twee implementaties voorgesteld. De eerste in Apache Spark (zie \ref{spark}) en een tweede in Apache Mahout (zie \ref{mahout}). Beiden werken bovenop het Hadoop filesysteem om sneller te kunnen werken en mogelijk gemakkelijker te schalen bij groter wordende datacollecties. Bovendien ondersteunen beide systemen enkele vormen van clustering. 

\subsection{Implementatie van K-Means met Apache Spark}\label{spark}
Apache Hadoop \cite{hadoop} is een open-source framework dat oplossingen biedt voor big data, samen met uitgebreide verwerkings- en analysetools. Hadoop bestaat uit twee grote componenten: HDFS (Hadoop Distributed File System) en Map Reduce \cite{Dean}. HDFS is een schaalbaar, effici\"ent en replica-based opslagmedium voor data. Het is gebaseerd op een master-slave architectuur waar \quotes{namenode} de master is en de \quotes{datanodes} de slaves. De slaves bevatten ook de effectieve data. 

Map Reduce zorgt ervoor dat gerepliceerde data snel en parallel verwerkt kan worden volgens map en reduce fases. \textit{Map} is de fase die gebruikt wordt om in parallel de delen van het gedistribueerd systeem op te halen via verschillende \textit{mappers}. De outputs van de mappers wordt door sorteer- en shufflealgoritmes gehaald om in de \textit{reduce} fase de data te aggregeren en het resultaat te vinden van het initieel probleem.

Apache Spark werd ontwikkeld in het UC Berkeley AMPLab en werd open-source in 2010. Het is ontworpen voor effici\"ente gegevensverwerking en bied een gebruiksvriendelijke interface aan om zo tot betere oplossingen te komen voor big data toepassingen. Spark heeft sinds 2012 bovendien een set robuuste en schaalbare leeralgoritmes onder de naam \quotes{MLlib} en ondersteunt een veel breder gamma aan applicaties dan Map Reduce, maar behoud zijn automatische fouttolerantie.
 
Spark werkt met RDDs (Resilient distributed Datsets) om efficiente applicaties te ondersteunen. RDDs kunnen opgeslagen worden in het geheugen tussen query's zonder dat replicatie nodig is. Indien data verloren gaat wordt deze opnieuw opgebouwd gebruik makend van \textit{lineage}: elke RDD herinnert hoe het gebouwd is van een andere dataset en kan zichzelf zo herbouwen. Op die manier presteert Spark beter dan alle bestaande modellen.

Apache spark biedt in zijn MlLib een k-means algoritme aan. Dit werd getest in \cite{Gopalani2015a} en werkt tot drie keer sneller dan hetzelfde algoritme gebruik makend van MapReduce. De implemenatie is hier wel zo dat er geen gebruik kan gemaakt worden van een alternatieve afstandsfunctie. Om voor de snelheid van Apache Spark te kiezen zijn we dus verplicht over te stappen naar de Euclidische afstandsfunctie.

Een vergelijking in tijd om de eerste 25 clusters te maken bij de k-means implementatie van Apache Spark (met 1000 features en 100 iteraties) en het eenvoudige algoritme dat hiervoor reeds gebruikt werd kan gevonden worden in figuur \ref{fig:sparkvskmeans}. 

\begin{figure}[h]
	\caption{Vergelijking tussen de tijd om k-means clustering door te voeren in Apache Spark en binnen een eenvoudig algoritme}
	\label{fig:sparkvskmeans}	
	\includegraphics[width=\textwidth]{fig/sparkvsclustering.png}
\end{figure}

De grafiek van het eenvoudige algoritme is dezelfde als in figuur \ref{fig:clustering-10000-exp}, maar we merken vooral de quasi lineaire tijd van Apache Spark op. Deze doet er gemiddeld 44 seconden over om een clustering uit te voeren in de eerste 50 clusters, terwijl het eenvoudige algoritme al 377 seconden nodig had om cluster 25 te vormen. Door Apache Spark te gebruiken kunnen we de rekentijd dus flink beperken, maar ten koste van het gebruik van de cosine distance.

\subsection{Implementatie van K-Means met Apache Mahout}\label{mahout}


\subsection{Andere clustertechnieken}

\subsection{Conclusie voor clustering}


\section{Aanbevelingssysteem}
Het gebruikte aanbevelingssysteem is een klassieke content-based recommender, gebouwd in Lenskit \cite{Ekstrand2011a}. In plaats van de manueel ingegeven tags wordt gebruik gemaakt van de verworven informatie uit zowel classificatie als clustering. Deze informatie wordt doorgegeven aan de recommender onder vorm van tags. Zo krijgt elk item een tag voor de hoofdcategorie, een tag voor de subcategorie en een tag met de cluster waartoe het item behoort. Met deze data kan het content-based systeem gelijkaardige items vinden voor een bepaalde gebruiker. De resultaten en performantie van dit aanbevelingssysteem worden in hoofdstuk \ref{resultaten} besproken.