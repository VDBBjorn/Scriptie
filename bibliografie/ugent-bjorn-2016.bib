Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Bengio2004,
abstract = {estimation-methods paper},
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {:home/bjorn/Downloads/grandvalet04a.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,k-fold cross-validation,statistical comparisons,variance estimators},
pages = {1089--1105},
pmid = {12646250},
title = {{No Unbiased Estimator of the Variance of K-Fold Cross-Validation}},
volume = {5},
year = {2004}
}
@article{Hall2009,
abstract = {Abstract More than twelve years have elapsed since the first public release of WEKA . In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread ... $\backslash$n},
author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H},
doi = {10.1145/1656274.1656278},
file = {:home/bjorn/Downloads/p10-hall.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM SIGKDD Explorations},
number = {1},
pages = {10--18},
title = {{The WEKA data mining software}},
url = {http://portal.acm.org/citation.cfm?doid=1656274.1656278$\backslash$npapers2://publication/doi/10.1145/1656274.1656278},
volume = {11},
year = {2009}
}
@article{Ekstrand2011,
author = {Ekstrand, Michael D. and Ludwig, Michael and Kolb, Jack and Riedl, John T.},
doi = {10.1145/2043932.2044001},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ekstrand et al. - 2011 - LensKit.pdf:pdf},
isbn = {9781450306836},
journal = {Proceedings of the fifth ACM conference on Recommender systems - RecSys '11},
pages = {349},
title = {{LensKit}},
url = {http://dl.acm.org/citation.cfm?doid=2043932.2044001},
year = {2011}
}
@article{Harper2015,
author = {Harper, F. Maxwell and Konstan, Joseph A.},
doi = {10.1145/2827872},
issn = {21606455},
journal = {ACM Transactions on Interactive Intelligent Systems},
month = {dec},
number = {4},
pages = {1--19},
title = {{The MovieLens Datasets}},
url = {http://dl.acm.org/citation.cfm?doid=2866565.2827872},
volume = {5},
year = {2015}
}
@article{Cheng2013,
abstract = {Wikification, commonly referred to as Disam- biguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific correspondingWikipedia pages. Previous ap- proaches to D2W focused on the use of lo- cal and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these meth- ods fail (often, embarrassingly), when some level of text understanding is needed to sup- port Wikification. In this paper we introduce a novel approach to Wikification by incorpo- rating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Lin- ear Programming (ILP) formulation of Wik- ification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant im- provements in bothWikification and the TAC Entity Linking task.},
author = {Cheng, Xiao and Roth, Dan},
file = {:home/bjorn/Downloads/ChengRo13.pdf:pdf},
isbn = {9781937284978},
journal = {Empirical Methods in Natural Language Processing},
number = {October},
pages = {1787--1796},
title = {{Relational Inference for Wikification}},
url = {http://aclweb.org/anthology/D/D13/D13-1184.pdf},
year = {2013}
}
@misc{ROBERTSON1977,
abstract = {The principle that, for optimal retrieval, documents should be ranked in order of the probability of relevance or usefulness has been brought into question by Cooper. It is shown that the principle can be justified under certain assumptions, but that in cases where these assumptions do not hold, the principle is not valid. The major problem appears to lie in the way the principle considers each document independently of the rest. The nature of the information on the basis of which the system decides whether or not to retrieve the documents determines whether the document-by-document approach is valid.},
author = {ROBERTSON, S.E.},
booktitle = {Journal of Documentation},
doi = {10.1108/eb026647},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ROBERTSON - 1977 - the Probability Ranking Principle in Ir.pdf:pdf},
isbn = {1558604545},
issn = {0022-0418},
number = {4},
pages = {294--304},
title = {{the Probability Ranking Principle in Ir}},
url = {http://www.emeraldinsight.com/doi/abs/10.1108/eb026647},
volume = {33},
year = {1977}
}
@article{Nasa2012,
author = {Nasa, Divya},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nasa - 2012 - Text Mining Techniques- A Survey.pdf:pdf},
keywords = {applications,benefits and limitations,classification,clustering,information extraction,summarization,techniques,text mining framework,visualization},
number = {4},
pages = {1--5},
title = {{Text Mining Techniques- A Survey}},
volume = {2},
year = {2012}
}
@article{SEEWALD,
abstract = {In this paper, we discuss grading, a meta-classification technique that tries to identify and correct incorrect predictions at the base level. While stacking uses the predictions of the base classifiers as meta-level attributes, we use graded predictions (i.e., predictions that have been marked as correct or incorrect) as meta-level classes. For each base classifier, one meta classifier is learned whose task is to predict when the base classifier will err. Hence, just like stacking may be viewed as a generalization of voting, grading may be viewed as a generalization of selection by cross-validation and therefore fills a conceptual gap in the space of meta-classification schemes. Our experimental evaluation shows that this technique results in a performance gain that is quite comparable to that achieved by stacking, while both, grading and stacking outperform their simpler counter-parts voting and selection by cross-validation.},
author = {Seewald, AK and F{\"{u}}rnkranz, J},
doi = {10.1007/3-540-44816-0_12},
isbn = {3540425810},
issn = {16113349},
journal = {Advances in Intelligent Data Analysis. Lecture Notes in Computer Science},
keywords = {Algorithme apprentissage,Algoritmo aprendizaje,Clasificador,Classificateur,Classifier,Cross validation,Evaluaci{\'{o}}n prestaci{\'{o}}n,Evaluation performance,Learning algorithm,M{\'{e}}taclassification,Performance evaluation,Validaci{\'{o}}n cruzada,Validation crois{\'{e}}e,Vote,Voting,Voto},
pages = {115--124},
publisher = {Springer},
title = {{An evaluation of grading classifiers}},
url = {http://link.springer.com/chapter/10.1007/3-540-44816-0{\_}12},
volume = {2189},
year = {2001}
}
@article{Salton1988,
author = {Salton, Gerard and Buckley, Christopher},
doi = {10.1016/0306-4573(88)90021-0},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Buckley - 1988 - Term-weighting approaches in automatic text retrieval.pdf:pdf},
issn = {03064573},
journal = {Information Processing {\&} Management},
month = {jan},
number = {5},
pages = {513--523},
title = {{Term-weighting approaches in automatic text retrieval}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0306457388900210},
volume = {24},
year = {1988}
}
@article{Chinchor1997,
abstract = {INTRODUCTION The Named Entity task consists of three subtasks (entity names, temporal expressions, number expressions). The expressions to be annotated are "unique identifiers" of entities (organizations, persons, locations), times (dates, times), and quantities (monetary values, percentages)...},
author = {Chinchor, Nancy and Robinson, Patty},
file = {:home/bjorn/Downloads/document(2).pdf:pdf},
journal = {Proceedings of the Sixth Message Understanding Conference MUC6},
number = {September},
pages = {21},
title = {{MUC-7 Named Entity Task Definition}},
year = {1997}
}
@article{Shani2011,
abstract = {Recommender systems are now popular both commercially and in the research community, where many approaches have been suggested for providing recommendations. In many cases a system designer that wishes to employ a rec- ommendation system must choose between a set of candidate approaches. A first step towards selecting an appropriate algorithm is to decide which properties of the application to focus upon when making this choice. Indeed, recommendation sys- tems have a variety of properties that may affect user experience, such as accuracy, robustness, scalability, and so forth. In this paper we discuss howto compare recom- menders based on a set of properties that are relevant for the application.We focus on comparative studies, where a fewalgorithms are compared using some evaluation metric, rather than absolute benchmarking of algorithms.We describe experimental settings appropriate for making choices between algorithms.We review three types of experiments, starting with an offline setting, where recommendation approaches are compared without user interaction, then reviewing user studies, where a small group of subjects experiment with the system and report on the experience, and fi- nally describe large scale online experiments, where real user populations interact with the system. In each of these cases we describe types of questions that can be answered, and suggest protocols for experimentation.We also discuss how to draw trustworthy conclusions from the conducted experiments. We then review a large set of properties, and explain how to evaluate systems given relevant properties.We also survey a large set of evaluation metrics in the context of the property that they evaluate.},
author = {Shani, Guy and Gunawardana, Asela},
doi = {10.1007/978-0-387-85820-3_8},
file = {:home/bjorn/Downloads/EvaluationMetrics.TR.pdf:pdf},
isbn = {9780387858197},
issn = {08909369},
journal = {Recommender systems handbook},
pages = {257--298},
pmid = {9765210},
title = {{Evaluating recommendation systems}},
url = {http://link.springer.com/chapter/10.1007/978-0-387-85820-3{\_}8},
year = {2011}
}
@article{Sukanya2012,
abstract = {Knowledge discovery from textual database refers generally to the process of extracting interesting or non-retrieval patterns or knowledge from unstructured text documents. This is also called as text data mining or knowledge discovery. It can be viewed as an extension of data mining or knowledge from structured databases. At present, the stored information is increasing tremendously day by day. This is unstructured form so we can not extract the needed information. Some data mining techniques are used to extract the useful information from text documents, such as classification, clustering, visualization and information extraction. Here framework of text mining with techniques is discussed as well as benefits and limitations of text mining have been discussed.},
author = {Sukanya, M. and Biruntha, S.},
doi = {10.1109/ICACCCT.2012.6320784},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sukanya, Biruntha - 2012 - Techniques on text mining.pdf:pdf},
isbn = {9781467320467},
journal = {Proceedings of 2012 IEEE International Conference on Advanced Communication Control and Computing Technologies, ICACCCT 2012},
keywords = {Text mining framework,benefits and limitations,classification,clustering,information extraction,visualization},
number = {978},
pages = {269--271},
title = {{Techniques on text mining}},
year = {2012}
}
@article{Dash1997,
abstract = {Feature selection has been the focus of interest for quite some time and much work has been done.With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications.},
author = {Dash, M. and Liu, H.},
doi = {10.3233/IDA-1997-1302},
file = {:home/bjorn/Downloads/dash-1997.pdf:pdf},
isbn = {1088-467X},
issn = {1088467X},
journal = {Intelligent Data Analysis},
keywords = {Classification,Feature selection,Framework},
number = {3},
pages = {131--156},
pmid = {21600290},
title = {{Feature selection for classification}},
volume = {1},
year = {1997}
}
@article{Zhang2004,
author = {Zhang, H},
doi = {10.1016/j.patrec.2005.12.001},
file = {:home/bjorn/Downloads/Flairs04-097.pdf:pdf},
isbn = {978-1-57735-201-3},
issn = {01678655},
journal = {Aa},
keywords = {Copyright {\textcopyright} 2004, American Association for Artific},
title = {{The optimality of naive Bayes}},
url = {http://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf},
year = {2004}
}
@article{Likas2011,
author = {Likas, Aristidis and Vlassis, Nikos and Verbeek, Jakob},
file = {:home/bjorn/Downloads/verbeek01tr.pdf:pdf},
journal = {ISA technical report series},
title = {{The global k-means clustering algorithm Intelligent Autonomous Systems}},
year = {2011}
}
@article{Dean,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
eprint = {10.1.1.163.5292},
file = {:home/bjorn/Downloads/mapreduce-osdi04.pdf:pdf},
isbn = {9781595936868},
issn = {00010782},
journal = {Proceedings of 6th Symposium on Operating Systems Design and Implementation},
pages = {137--149},
pmid = {11687618},
title = {{MapReduce: Simplied Data Processing on Large Clusters}},
year = {2004}
}
@article{Herlocker2004,
abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.},
archivePrefix = {arXiv},
arxivId = {50},
author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Terveen, Loren G. and Riedl, John T.},
doi = {10.1145/963770.963772},
eprint = {50},
file = {:home/bjorn/Downloads/evaluating-TOIS-20041.pdf:pdf},
isbn = {1011459637},
issn = {1046-8188},
journal = {ACM Transactions on Information Systems (TOIS)},
keywords = {Collaborative filtering,evaluation,metrics,recommender systems},
number = {1},
pages = {5--53},
pmid = {18244404},
title = {{Evaluating collaborative filtering recommender systems}},
url = {http://portal.acm.org/citation.cfm?id=963770.963772},
volume = {22},
year = {2004}
}
@article{Montalvo2015,
abstract = {In this article, we present a new algorithm for clustering a bilingual collection of comparable news items in groups of specific topics. Our hypothesis is that named entities (NEs) are more informative than other features in the news when clustering fine grained topics. The algorithm does not need as input any information related to the number of clusters, and carries out the clustering only based on information regarding the shared named entities of the news items. This proposal is evaluated using different data sets and outperforms other state-of-the-art algorithms, thereby proving the plausibility of the approach. In addition, because the applicability of our approach depends on the possibility of identifying equivalent named entities among the news, we propose a heuristic system to identify equivalent named entities in the same and different languages, thereby obtaining good performance.},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {Montalvo, Soto and Mart{\'{i}}nez, Raquel and Fresno, V{\'{i}}ctor and Delgado, Agust{\'{i}}n},
doi = {10.1002/asi.23175},
eprint = {0803.1716},
file = {:home/bjorn/Downloads/asi23175.pdf:pdf},
isbn = {9783848215430},
issn = {23301635},
journal = {Journal of the Association for Information Science and Technology},
keywords = {natural language processing},
mendeley-tags = {natural language processing},
month = {feb},
number = {2},
pages = {363--376},
pmid = {502955140},
title = {{Exploiting named entities for bilingual news clustering}},
url = {http://doi.wiley.com/10.1002/asi.23175},
volume = {66},
year = {2015}
}
@article{Ray,
author = {Ray, Siddheswar and Turi, Rose H},
file = {:home/bjorn/Downloads/cal99.pdf:pdf},
keywords = {clustering,colour image segmentation,inter-class distance,intra-cluster distance,k- means},
title = {{Determination of Number of Clusters in}}
}
@book{Han2012,
author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
edition = {3 edition},
isbn = {0123814790},
pages = {744},
title = {{Data Mining: Concepts and Techniques}},
year = {2012}
}
@article{Weale2009,
author = {Weale, Timothy and Brew, Chris and Fosler-lussier, Eric},
doi = {10.3115/1699765.1699769},
file = {:home/bjorn/Downloads/p28-weale.pdf:pdf},
isbn = {1932432558},
journal = {In Proceedings of the Workshop on the People's Web Meets NLP},
number = {August},
pages = {28--31},
title = {{Using the wiktionary graph structure for synonym detection}},
year = {2009}
}
@article{McCallum1998,
abstract = {Recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes—providing on average a 27{\%} reduction in error over the multi-variate Bernoulli model at any vocabulary size. Introduction},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {McCallum, Andres and Nigam, Kamal},
doi = {10.1.1.46.1529},
eprint = {0-387-31073-8},
file = {:home/bjorn/Downloads/10.1.1.65.9324.pdf:pdf},
isbn = {0897915240},
issn = {0343-6993},
journal = {AAAI/ICML-98 Workshop on Learning for Text Categorization},
pages = {41--48},
pmid = {20236947},
title = {{A Comparison of Event Models for Naive Bayes Text Classification}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.9324{\&}rep=rep1{\&}type=pdf},
year = {1998}
}
@article{Gopalani2015a,
author = {Gopalani, Satish and Arora, Rohan},
file = {:home/bjorn/Downloads/pxc3900531.pdf:pdf},
keywords = {big data,hadoop,hdfs,k-means,machine learning,mahout,map reduce,mlib,spark},
number = {1},
pages = {8--11},
title = {{Comparing Apache Spark and Map Reduce with Performance Analysis using K-Means}},
volume = {113},
year = {2015}
}
@article{Everything2012,
author = {Linden, G. and Smith, B. and York, J.},
doi = {10.1109/MIC.2003.1167344},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Linden, Smith, York - 2003 - Amazon.com recommendations item-to-item collaborative filtering.pdf:pdf},
issn = {1089-7801},
journal = {IEEE Internet Computing},
month = {jan},
number = {1},
pages = {76--80},
title = {{Amazon.com recommendations: item-to-item collaborative filtering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1167344},
volume = {7},
year = {2003}
}
@article{Science2013,
author = {Kodinariya, Trupti and Makwana, Prashant},
file = {:home/bjorn/Downloads/V1I6-0015.pdf:pdf},
journal = {International Journal of Advance Research in Computer Science and Management Studies},
keywords = {akaike,bayesian inference criterion,clustering,cross-validation,elbow method,jump,method,number of cluster,s information criterion,silhouette},
number = {6},
pages = {90--95},
title = {{Review on determining number of Cluster in K-Means Clustering}},
volume = {1},
year = {2013}
}
@article{Rigsbergen1986,
abstract = {Implicit in many information retrieval model is logic. These logics are hardly ever formalised. This paper formalises non-classical logic underlying information retrieval. It shows how a particular conditional logic is the 'right' logic to do Information Retrieval. Its relationship to exisitng retrieval mechanisms is investigated. The semantics of the logic are expressed in probability theory, and evaluated through a possible-world analysis, thus establishing an intensional logic. In doing so, we motivate a new principle, the logic uncertainty principle, which gives a measure of the uncertainty associated with an inference.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rigsbergen, Van V. J.},
doi = {10.1093/comjnl/29.6.481},
eprint = {arXiv:1011.1669v3},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rigsbergen - 1986 - A non classical logic for Information retrieval.pdf:pdf},
isbn = {9780874216561},
issn = {0010-4620},
journal = {The Computer Journal},
keywords = {icle},
month = {jun},
number = {6},
pages = {481--485},
pmid = {25246403},
title = {{A non classical logic for Information retrieval}},
url = {http://comjnl.oupjournals.org/cgi/doi/10.1093/comjnl/29.6.481},
volume = {29},
year = {1986}
}
@article{Forman2003,
abstract = {Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives—accuracy, F-measure, precision, and recall—since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair—e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.},
author = {Forman, George},
doi = {10.1162/153244303322753670},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Forman - 2003 - An Extensive Empirical Study of Feature Selection Metrics for Text Classification.pdf:pdf},
isbn = {1558604863},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {ROC,document categorization,supervised learning,support vector machines},
pages = {1289--1305},
title = {{An Extensive Empirical Study of Feature Selection Metrics for Text Classification}},
volume = {3},
year = {2003}
}
@misc{Tibshirani2001,
abstract = {We propose a method (the ‘gap statistic') for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
author = {Tibshirani, R and Walther, G and Hastie, T},
booktitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
doi = {10.1111/1467-9868.00293},
file = {:home/bjorn/Downloads/gap.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
pages = {411--423},
pmid = {306526},
title = {{Estimating the number of clusters in a data set via the gap statistic}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00293/abstract},
volume = {63},
year = {2001}
}
@article{Tan2005,
abstract = {Cluster analysis divides data into groups (clusters) that aremeaningful, useful, or both. If meaningful groups are the goal, then the clusters should capture the natural structure of the data. In some cases, however, cluster analysis is only a useful starting point for other purposes, such as data summarization. Whether for understanding or utility, cluster analysis has long played an important role in a wide variety of fields: psychology and other social sciences, biology, statistics, pattern recognition, information retrieval, machine learning, and data mining. There have been many applications of cluster analysis to practical prob- lems. We provide some specific examples, organized by whether the purpose of the clustering is understanding or utility.},
author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
doi = {10.1016/0022-4405(81)90007-8},
file = {:home/bjorn/Downloads/ch8.pdf:pdf},
isbn = {0321321367},
issn = {00224405},
journal = {Introduction to Data Mining},
pages = {Chapter 8},
pmid = {21635741},
title = {{Chap 8 : Cluster Analysis: Basic Concepts and Algorithms}},
url = {http://www-users.cs.umn.edu/{~}kumar/},
year = {2005}
}
@article{Bennett2007,
abstract = {In October, 2006 Netflix released a dataset containing 100 million anonymous movie ratings and challenged the data mining, machine learning and computer science communities to develop systems that could beat the accuracy of its recommendation system, Cinematch. We briefly describe the challenge itself, review related work and efforts, and summarize visible progress to date. Other potential uses of the data are outlined, including its application to the KDD Cup 2007.},
author = {Bennett, James and Lanning, Stan},
doi = {10.1145/1562764.1562769},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bennett, Lanning - 2007 - The Netflix Prize.pdf:pdf},
isbn = {9781595938343},
issn = {1554351X},
journal = {KDD Cup and Workshop},
keywords = {machine learning,netflix prize,rmse},
pages = {3--6},
title = {{The Netflix Prize}},
url = {http://su-2010-projekt.googlecode.com/svn-history/r157/trunk/literatura/bennett2007netflix.pdf},
year = {2007}
}
@article{Ceska2009,
abstract = {This paper explores the influence of text pre- processing techniques on plagiarism detection. We examine stop-word removal, lemmatization, number replacement, synonymy recognition, and word generalization. We also look into the in- fluence of punctuation and word-order within N-grams. All these techniques are evaluated according to their impact on F1-measure and speed of execution. Our experiments were per- formed on a Czech corpus of plagiarized docu- ments about politics. At the end of this paper, we propose what we consider to be the best com- bination of text pre-processing techniques.},
author = {Ceska, Zdenek and Fox, Chris},
file = {:home/bjorn/Downloads/R09-1011.pdf:pdf},
issn = {13138502},
journal = {International Conference RANLP 2009},
keywords = {copy detection,lemmatization,natural language processing,plagiarism,stop-words,synonymy,thesaurus,wordnet},
pages = {55--59},
title = {{The Influence of Text Pre-processing on Plagiarism Detection}},
year = {2009}
}
@article{McNamee2004,
abstract = {The caliber of available translation resources is a key factor in the accuracy of a system for - information retrieval ( and 2002a},
author = {McNamee, Paul and Mayfield, James},
doi = {10.1023/B:INRT.0000009441.78971.be},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McNamee, Mayfield - 2004 - Character N-Gram Tokenization for European Language Text Retrieval.pdf:pdf},
issn = {1386-4564},
journal = {Information Retrieval},
keywords = {character n -grams,cross language,cross-language information retrieval,european languages,evaluation forum,language-neutral retrieval},
number = {1-2},
pages = {73--97},
title = {{Character N-Gram Tokenization for European Language Text Retrieval}},
url = {http://www.springerlink.com/openurl.asp?id=doi:10.1023/B:INRT.0000009441.78971.be},
volume = {7},
year = {2004}
}
@misc{tika,
author = {{The Apache Software Foundation}},
title = {{Apache Tika}},
url = {http://tika.apache.org/},
urldate = {2016-03-04}
}
@article{Powers2007,
author = {Powers, David M W},
file = {:home/bjorn/Downloads/TRRA{\_}2007.pdf:pdf},
keywords = {Bookmaker Informedness and Markedness,Chi-Squared Significance,Cohen Kappa,Evenness,F-Factor,Log-Likelihood Significance,Matthews Correlation,Pearson Correlation,Precision,Rand Accuracy,Recall},
number = {December},
pages = {24},
title = {{Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness {\&} Correlation}},
year = {2007}
}
@article{Ekstrand2011a,
abstract = {Recommender systems research is being slowed by the diffi- culty of replicating and comparing research results. Published research uses various experimental methodologies andmetrics that are difficult to compare. It also often fails to sufficiently document the details of proposed algorithms or the eval- uations employed. Researchers waste time reimplementing well-known algorithms, and the new implementations may miss key details from the original algorithm or its subsequent refinements. When proposing new algorithms, researchers should compare them against finely-tuned implementations of the leading prior algorithms using state-of-the-art evaluation methodologies. With few exceptions, published algorithmic improvements in our field should be accompanied by working code in a standard framework, including test harnesses to reproduce the described results. To that end, we present the design and freely distributable source code of LensKit, a flexible platform for reproducible recommender systems research. LensKit provides carefully tuned implementations of the leading collaborative filtering algorithms, APIs for common recommender system use cases, and an evaluation framework for performing reproducible offline evaluations of algorithms. We demonstrate the utility of LensKit by repli- cating and extending a set of prior comparative studies of recommender algorithms — showing limitations in some of the original results—and by investigating a question recently raised by a leader in the recommender systems community on problems with error-based prediction evaluation.},
author = {Ekstrand, Michael D and Ludwig, Michael and Konstan, Joseph a and Riedl, John T},
doi = {10.1145/2043932.2043958},
file = {:home/bjorn/Downloads/p133-ekstrand.pdf:pdf},
isbn = {9781450306836},
journal = {Proceedings of the 5th ACM conference on Recommender systems - RecSys '11},
keywords = {evaluation,implementation,recommender systems},
pages = {133--140},
title = {{Rethinking the Recommender Research Ecosystem : Categories and Subject Descriptors}},
year = {2011}
}
@article{Hotho2005,
abstract = {The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.},
author = {Hotho, Andreas and N{\"{u}}rnberger, Andreas and Paa{\ss}, Gerhard},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hotho, N{\"{u}}rnberger, Paa{\ss} - 2005 - A Brief Survey of Text Mining(2).pdf:pdf},
issn = {0175-1336},
journal = {LDV-Forum},
keywords = {Survey,Text Mining},
number = {1},
pages = {19--62},
title = {{A Brief Survey of Text Mining}},
volume = {20},
year = {2005}
}
@article{Huang2008,
abstract = {Clustering is a useful technique that organizes a large quan- tity of unordered text documents into a small number of meaningful and coherent clusters, thereby providing a ba- sis for intuitive and informative navigation and browsing mechanisms. Partitional clustering algorithms have been recognized to be more suitable as opposed to the hierar- chical clustering schemes for processing large datasets. A wide variety of distance functions and similarity measures have been used for clustering, such as squared Euclidean distance, cosine similarity, and relative entropy. In this paper, we compare and analyze the effectiveness of these measures in partitional clustering for text docu- ment datasets. Our experiments utilize the standard K- means algorithm and we report results on seven text doc- ument datasets and five distance/similarity measures that have been most commonly used in text clustering.},
author = {Huang, Anna},
file = {:home/bjorn/Downloads/pg049{\_}Similarity{\_}Measures{\_}for{\_}Text{\_}Document{\_}Clustering(1).pdf:pdf},
journal = {Proceedings of the Sixth New Zealand},
number = {April},
pages = {49--56},
title = {{Similarity measures for text document clustering}},
url = {http://nzcsrsc08.canterbury.ac.nz/site/proceedings/Individual{\_}Papers/pg049{\_}Similarity{\_}Measures{\_}for{\_}Text{\_}Document{\_}Clustering.pdf},
year = {2008}
}
@article{Hyndman2006,
author = {Hyndman, Rob J and Koehler, Anne B},
doi = {10.1016/j.ijforecast.2006.03.001},
file = {:home/bjorn/Downloads/1-s2.0-S0169207006000239-main.pdf:pdf},
keywords = {forecast accuracy,forecast error measures,forecast evaluation,m-competition,mean absolute scaled error},
pages = {679--688},
title = {{Another look at measures of forecast accuracy}},
volume = {22},
year = {2006}
}
@book{Fallis2013,
abstract = {Information Retrieval: Algorithms and Heuristics is a comprehensive introduction to the study of information retrieval covering both effectiveness and run-time performance. The focus of the presentation is on algorithms and heuristics used to find documents relevant to the user request and to find them fast. Through multiple examples, the most commonly used algorithms and heuristics needed are tackled. To facilitate understanding and applications, introductions to and discussions of computational linguistics, natural language processing, probability theory and library and computer science are provided. While this text focuses on algorithms and not on commercial product per se, the basic strategies used by many commercial products are described. Techniques that can be used to find information on the Web, as well as in other large information collections, are included.  This volume is an invaluable resource for researchers, practitioners, and students working in information retrieval and databases. For instructors, a set of Powerpoint slides, including speaker notes, are available online from the authors.},
address = {Boston, MA},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Grossman, David A. and Frieder, Ophir},
booktitle = {Aging},
doi = {10.1007/978-1-4615-5539-1},
eprint = {arXiv:1011.1669v3},
isbn = {0792382714},
issn = {19454589},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {254},
pmid = {25246403},
publisher = {Springer US},
series = {The Kluwer International Series in Engineering and Computer Science},
title = {{Information Retrieval: Algorithms and Heuristics}},
url = {https://books.google.com/books?id=JZnnUqIbpqAC{\&}pgis=1},
volume = {461},
year = {1998}
}
@article{Milne2007,
abstract = {Domain-specific thesauri are high-cost, high-maintenance, high-value knowledge structures. We show how the classic thesaurus structure of terms and links can be mined automatically from Wikipedia. In a comparison with a professional thesaurus for agriculture we find that Wikipedia contains a substantial proportion of its concepts and semantic relations; furthermore it has impressive coverage of contemporary documents in the domain. Thesauri derived using our techniques capitalize on existing public efforts and tend to reflect contemporary language usage better than their costly, painstakingly-constructed manual counterparts},
author = {Milne, David and Medelyan, Olena and Witten, Ian H.},
doi = {10.1109/WI.2006.119},
file = {:home/bjorn/Downloads/06-DM-OM-IHW-wikipedia{\_}vs{\_}agrovoc.pdf:pdf},
isbn = {0769527477},
journal = {Proceedings - 2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings), WI'06},
pages = {442--448},
title = {{Mining domain-specific thesauri from Wikipedia: A case study}},
year = {2007}
}
@article{Friburger2002,
author = {Friburger, N and Maurel, D},
file = {:home/bjorn/Downloads/10.1.1.18.5008.pdf:pdf},
journal = {Proceedings of the workshop {\{}Mathematical/Formal{\}} Methods in Information Retrieval {\{}(MFIR'2002){\}} at the 25 {\{}thACM{\}} {\{}SIGIR{\}} Conference},
keywords = {clustering,ie,ir,proper names,similarity},
pages = {155--167},
title = {{Textual Similarity Based on Proper Names}},
year = {2002}
}
@article{Zaharia2010,
abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However; Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs; and can be used to interactively query a 39 GB dataset with sub-second response time.; as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals; most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
doi = {10.1007/s00256-009-0861-0},
eprint = {arXiv:1011.1669v3},
file = {:home/bjorn/Downloads/hotcloud{\_}spark.pdf:pdf},
isbn = {1432-2161 (Electronic)$\backslash$n0364-2348 (Linking)},
issn = {03642348},
journal = {HotCloud'10 Proceedings of the 2nd USENIX conference on Hot topics in cloud computing},
pages = {10},
pmid = {20205351},
title = {{Spark : Cluster Computing with Working Sets}},
year = {2010}
}
@book{Bogers2009b,
abstract = {Social bookmarking websites allow users to store, organize, and search bookmarks of web pages. Users of these services can an- notate their bookmarks by using informal tags and other metadata, such as titles, descriptions, etc. In this paper, we focus on the task of item recommendation for social bookmarking websites, i.e. pre- dicting which unseen bookmarks a user might like based on his or her profile. We examine how we can incorporate the tags and other metadata into a nearest-neighbor collaborative filtering (CF) algo- rithm, by replacing the traditional usage-based similarity metrics by tag overlap, and by fusing tag-based similarity with usage-based similarity. In addition, we perform experiments with content-based filtering by using the metadata content to recommend interesting items. We generate recommendations directly based on Kullback- Leibler divergence of the metadata language models, and we ex- plore the use of this metadata in calculating user and item simi- larities. We perform our experiments on three data sets from two different domains: Delicious, CiteULike and BibSonomy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bogers, Toine and {Van Den Bosch}, Antal},
booktitle = {CEUR Workshop Proceedings},
doi = {10.1007/978-0-387-85820-3},
eprint = {arXiv:1011.1669v3},
file = {:home/bjorn/Downloads/ContentBasedRS.pdf:pdf},
isbn = {978-0-387-85819-7},
issn = {16130073},
keywords = {Collaborative filtering,Content-based filtering,Folksonomies,Recommender systems,Social bookmarking},
pages = {9--16},
pmid = {21607264},
title = {{Collaborative and content-based filtering for item recommendation on social bookmarking websites}},
volume = {532},
year = {2009}
}
@article{Liu2005,
abstract = {This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward-building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to demonstrate the use of feature selection in data mining. We conclude this work by identifying trends and challenges of feature selection research and development.},
author = {{Huan Liu} and {Lei Yu}},
doi = {10.1109/TKDE.2005.66},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huan Liu, Lei Yu - 2005 - Toward integrating feature selection algorithms for classification and clustering.pdf:pdf},
isbn = {1041-4347},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Categorizing framework,Classification,Clustering,Feature selection,Real-world applications,Unifying platform},
month = {apr},
number = {4},
pages = {491--502},
title = {{Toward integrating feature selection algorithms for classification and clustering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1401889},
volume = {17},
year = {2005}
}
@article{McCallum2000,
abstract = {Many important problems involve clustering large datasets. Although naive implementations of clustering are computa- tionally expensive, there are established efficient techniques for clustering when the dataset has either (1) a limited num- ber of clusters, (2) a low feature dimensionality, or (3) a small number of data points. However, there has been much less work on methods of efficiently clustering datasets that are large in all three ways at oncefor example, having millions of data points that exist in many thousands of di- mensions representing many thousands of clusters},
author = {McCallum, Andrew and Nigam, Kamal and Ungar, Lyle H},
doi = {10.1145/347090.347123},
file = {:home/bjorn/Downloads/canopy-kdd00.pdf:pdf},
isbn = {1581132336},
journal = {Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining KDD 00},
pages = {169--178},
title = {{Efficient clustering of high-dimensional data sets with application to reference matching}},
url = {http://portal.acm.org/citation.cfm?doid=347090.347123},
year = {2000}
}
@article{Gliozzo2005,
author = {Gliozzo, Alfio and Strapparava, Carlo},
doi = {10.3115/1654449.1654452},
file = {:home/bjorn/Downloads/document(1).pdf:pdf},
journal = {Proceedings of the ACL Workshop on Building and Using Parallel Texts},
number = {June},
pages = {9--16},
title = {{Cross language text categorization by acquiring multilingual domain models from comparable corpora}},
year = {2005}
}
@article{Sigletos2005,
author = {Sigletos, Georgios and Paliouras, Georgios and Spyropoulos, Constantine D and Hatzopoulos, Michalis},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sigletos et al. - 2005 - Combining Information Extraction Systems Using Voting and Stacked Generalization Georgios Sigletos Georgios Pal.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,information extraction,stacking,voting},
pages = {1751--1782},
title = {{Combining Information Extraction Systems Using Voting and Stacked Generalization Georgios Sigletos Georgios Paliouras}},
volume = {6},
year = {2005}
}
@article{Zesch2008,
abstract = {We introduce Wiktionary as an emerging lexical semantic re- source that can be used as a substitute for expert-made re- sources in AI applications. We evaluate Wiktionary on the pervasive task of computing semantic relatedness for English and German by means of correlation with human rankings and solving word choice problems. For the rst time, we ap- ply a concept vector based measure to a set of different con- cept representations like Wiktionary pseudo glosses, the rst paragraph of Wikipedia articles, English WordNet glosses, and GermaNet pseudo glosses. We show that: (i) Wiktionary is the best lexical semantic resource in the ranking task and performs comparably to other resources in the word choice task, and (ii) the concept vector based approach yields the best results on all datasets in both evaluations.},
author = {Zesch, Torsten and M{\"{u}}ller, Christof and Gurevych, Iryna},
file = {:home/bjorn/Downloads/AAAI08-137.pdf:pdf},
isbn = {9781577353683},
journal = {Proceedings of AAAI},
pages = {861--866},
title = {{Using Wiktionary for Computing Semantic Relatedness}},
volume = {8},
year = {2008}
}
@article{Kraaij1994,
abstract = {... Taking these considerations into account, six rule clusters were created for the Dutch Porter stemmer . ... Paice has compared three English stemmers : Lovins, Paice/Husk and Porter with the truncate stemmer . The trunc(n) stemmer reduces a word to its first n characters. ...},
author = {Kraaij, Wessel and Pohlmann, Ren{\'{e}}e},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kraaij, Pohlmann - 1994 - Porter`s stemming algorithm for Dutch.pdf:pdf},
journal = {Informatiewetenschap 1994: Wetenschapelijke bijdragen aan de derde STINFON Conferentie},
pages = {167--180},
title = {{Porter`s stemming algorithm for Dutch}},
year = {1994}
}
@misc{Burges2005,
abstract = {We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.},
author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
booktitle = {Proceedings of the 22nd international conference on Machine learning},
doi = {10.1145/1102351.1102363},
file = {:home/bjorn/Downloads/p89-burges.pdf:pdf},
isbn = {1595931805},
issn = {00243205},
pages = {89--96},
pmid = {16483612},
title = {{Learning to rank using gradient descent}},
volume = {pages},
year = {2005}
}
@article{Adomavicius2005,
abstract = { This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.},
archivePrefix = {arXiv},
arxivId = {3},
author = {Adomavicius, Gediminas and Tuzhilin, Alexander},
doi = {10.1109/TKDE.2005.99},
eprint = {3},
file = {:home/bjorn/Downloads/01423975.pdf:pdf},
isbn = {1066100802},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Collaborative filtering,Extensions to recommander systems,Rating estimation methods,Recommander systems},
number = {6},
pages = {734--749},
pmid = {1423975},
title = {{Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions}},
volume = {17},
year = {2005}
}
@article{Kittler1998,
author = {Kittler, Josef and Society, Ieee Computer and Hatef, Mohamad and Duin, Robert P W and Matas, Jiri},
doi = {10.1109/34.667881},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kittler et al. - 1998 - On Combining Classifiers.pdf:pdf},
isbn = {081867282X},
issn = {10514651},
number = {3},
pages = {226--239},
pmid = {20470429},
title = {{On Combining Classifiers}},
volume = {20},
year = {1998}
}
@article{Liu2010,
abstract = {Online news reading has become very popular as the web provides access to news articles from millions of sources around the world. A key challenge of news websites is to help users find the articles that are interesting to read. In this paper, we present our research on developing personalized news recommendation system in Google News. For users who are logged in and have explicitly enabled web history, the recommendation system builds profiles of users' news interests based on their past click behavior. To understand how users' news interests change over time, we first conducted a large-scale analysis of anonymized Google News users click logs. Based on the log analysis, we developed a Bayesian framework for predicting users' current news interests from the activities of that particular user and the news trends demonstrated in the activity of all users. We combine the content-based recommendation mechanism which uses learned user profiles with an existing collaborative filtering mechanism to generate personalized news recommendations. The hybrid recommender system was deployed in Google News. Experiments on the live traffic of Google News website demonstrated that the hybrid method improves the quality of news recommendation and increases traffic to the site.},
author = {Liu, Jiahui and Dolan, Peter and Pedersen, Er},
doi = {10.1145/1719970.1719976},
file = {:home/bjorn/Downloads/35599.pdf:pdf},
isbn = {9781605585154},
issn = {08955638},
journal = {Proceedings of the 15th international {\ldots}},
keywords = {新闻推荐,用户画像,短期兴趣,长期兴趣},
pages = {31--40},
title = {{Personalized news recommendation based on click behavior}},
url = {http://portal.acm.org/citation.cfm?id=1719970.1719976$\backslash$nhttp://users.cis.fiu.edu/{~}lli003/recsys/papers/hybrid/p31-liu.pdf},
year = {2010}
}
@book{Owen2011,
abstract = {A computer system that learns and adapts as it collects data is an extraordinarily interesting and powerful concept. With new technologies to capture, store, and process information, machine learning has moved from the academic edges of computer science to the middle of the mainstream. Mahout, an open source machine learning library, captures the core algorithms of recommendation systems, classification, and clustering in ready-to-use, scalable libraries. With Mahout, you can immediately apply the machine learning techniques that drive Amazon, Netflix, and other data-centric businesses to your own projects. Mahout in Action explores machine learning through Apache's scalable machine learning project, Mahout. Following real-world examples, it introduces practical use cases, and then illustrates how Mahout can be applied to solve them. It places particular focus on issues of scalability, and how to apply these techniques against large data sets using the Apache Hadoop framework. In this book, you'll use Mahout to dive into three practical applications of machine learning: Recommendations. Using group user history and preferences you can make accurate recommendations for individual users. This is an extremely powerful principle, because accurate recommendations are beneficial both to customers and vendors. Clustering. Learn to automatically discover logical groupings with groups of data or data sets, such as documents or lists. This technique is especially useful to search and data mining applications. Classification. Determining on the fly whether a thing fits a category based on its attributes and previous history can help instantaneously organize unstructured groups. For instance, you'll learn about filtering techniques that decide whether email messages should be considered "spam." Mahout in Action is written primarily for developers who need to become better practitioners of machine learning techniques. It is also appropriate for researchers who understand the techniques and want to understand how to apply them effectively at scale. It assumes familiarity with Java, and some basic grounding in machine learning techniques, but no previous exposure to Mahout is necessary.},
author = {Owen, Sean and Anil, Robin and Dunning, Ted and Friedman, Ellen},
booktitle = {Online},
doi = {citeulike-article-id:7544201},
file = {:home/bjorn/Downloads/Mahout.in.Action2011.Sean.Owen.pdf:pdf},
isbn = {9781935182689},
issn = {18125638},
pages = {375},
title = {{Mahout in Action}},
url = {http://www.manning.com/owen/},
year = {2011}
}
@article{Finkel2005,
abstract = {Most current statistical natural language process- ing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sam- pling, a simple Monte Carlo method used to per- form approximate inference in factored probabilis- tic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, andCRFs, it is possible to incorpo- rate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consis- tency constraints. This technique results in an error reduction of up to 9{\%} over state-of-the-art systems on two established information extraction tasks.},
author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
doi = {10.3115/1219840.1219885},
file = {:home/bjorn/Downloads/gibbscrf3.pdf:pdf},
issn = {02773791},
journal = {in Acl},
number = {1995},
pages = {363 -- 370},
title = {{Incorporating non-local information into information extraction systems by gibbs sampling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.8904},
year = {2005}
}
@article{Lloyd1982,
abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for quanta, , are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
author = {Lloyd, Stuart P.},
doi = {10.1109/TIT.1982.1056489},
file = {:home/bjorn/Downloads/lloyd-1982.pdf:pdf},
isbn = {00189448 (ISSN)},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {129--137},
title = {{Least Squares Quantization in PCM}},
volume = {28},
year = {1982}
}
@article{Fayyad1996,
abstract = {This paper presents a first step toward a unifying framework for Knowldege Discovery in Databases. We describe links between data mining,knowledge discovery and other related fields. We then define the KDD process and basic data mining algorithms, discuss application issues and conclude with an analysis of challenges facing practitioners in the field},
author = {Fayyad, UM and Piatetsky-Shapiro, G and Smyth, P},
doi = {10.1.1.27.363},
file = {:home/bjorn/Downloads/document.pdf:pdf},
isbn = {1-57735-004-9},
journal = {Proc 2nd Int Conf on Knowledge Discovery and Data Mining Portland OR},
pages = {82--88},
title = {{Knowledge Discovery and Data Mining: Towards a Unifying Framework.}},
url = {http://www.aaai.org/Papers/KDD/1996/KDD96-014},
year = {1996}
}
@misc{Porter1980,
abstract = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL....},
author = {Porter, M.F.},
booktitle = {Program: electronic library and information systems},
doi = {10.1108/eb046814},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Porter - 1980 - An algorithm for suffix stripping.pdf:pdf},
isbn = {1558604545},
issn = {0033-0337},
number = {3},
pages = {130--137},
pmid = {16143652},
title = {{An algorithm for suffix stripping}},
volume = {14},
year = {1980}
}
@incollection{Wilks1997,
author = {Wilks, Yorick},
doi = {10.1007/3-540-63438-X_1},
pages = {1--9},
title = {{Information extraction as a core language technology}},
url = {http://link.springer.com/10.1007/3-540-63438-X{\_}1},
year = {1997}
}
@book{Mattmann2011,
abstract = {Title: Tika In Action SubTitle: ; Volume: ; Serie: ; Edition: ; Authors: Mattmann, Chris A. And Zitting, Jukka L. ; Year: 2011 ; Pages: 257 ; Editor: ; Publisher: Manning ; ISBN: 978-1-234-56789-7 ; Keywords: Action; Tika ;},
author = {Mattmann, Chris a and Zitting, Jukka L},
booktitle = {Tika In Action},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mattmann, Zitting - 2011 - Tika In Action.pdf:pdf},
isbn = {978-1-234-56789-7},
keywords = {Action,Tika},
pages = {257},
title = {{Tika In Action}},
year = {2011}
}
@article{Manning2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manning, Christopher D. and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
eprint = {arXiv:1011.1669v3},
file = {:home/bjorn/Downloads/P14-5.pdf:pdf},
issn = {1098-6596},
journal = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
keywords = {icle},
pages = {55--60},
pmid = {25246403},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
volume = {53},
year = {2014}
}
@article{Sigletos2005,
author = {Sigletos, Georgios and Paliouras, Georgios and Spyropoulos, Constantine D and Hatzopoulos, Michalis},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spyropoulos - 2005 - Combining Information Extraction Systems Using Voting and Stacked Generalization Georgios Sigletos Georgios Paliour.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,information extraction,stacking,voting},
pages = {1751--1782},
title = {{Combining Information Extraction Systems Using Voting and Stacked Generalization Georgios Sigletos Georgios Paliouras}},
volume = {6},
year = {2005}
}
@misc{hadoop,
title = {{Apache Hadoop Documentation}},
url = {https://hadoop.apache.org/docs/r1.2.1/index.html},
urldate = {2016-05-01}
}
@article{Salton1975,
author = {Salton, G and Wong, A and Yang, C S},
doi = {http://doi.acm.org/10.1145/361219.361220},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Wong, Yang - 1975 - A vector space model for automatic indexing.pdf:pdf},
issn = {0001-0782},
journal = {Cacm},
keywords = {automatic indexing,automatic information retrieval,content analysis,document space},
number = {11},
pages = {613--620},
title = {{A vector space model for automatic indexing}},
url = {http://doi.acm.org/10.1145/361219.361220},
volume = {18},
year = {1975}
}
