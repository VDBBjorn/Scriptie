Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ray,
author = {Ray, Siddheswar and Turi, Rose H},
file = {:home/bjorn/Downloads/cal99.pdf:pdf},
keywords = {clustering,colour image segmentation,inter-class distance,intra-cluster distance,k- means},
title = {{Determination of Number of Clusters in}}
}
@book{Fallis2013,
abstract = {Information Retrieval: Algorithms and Heuristics is a comprehensive introduction to the study of information retrieval covering both effectiveness and run-time performance. The focus of the presentation is on algorithms and heuristics used to find documents relevant to the user request and to find them fast. Through multiple examples, the most commonly used algorithms and heuristics needed are tackled. To facilitate understanding and applications, introductions to and discussions of computational linguistics, natural language processing, probability theory and library and computer science are provided. While this text focuses on algorithms and not on commercial product per se, the basic strategies used by many commercial products are described. Techniques that can be used to find information on the Web, as well as in other large information collections, are included.  This volume is an invaluable resource for researchers, practitioners, and students working in information retrieval and databases. For instructors, a set of Powerpoint slides, including speaker notes, are available online from the authors.},
address = {Boston, MA},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Grossman, David A. and Frieder, Ophir},
booktitle = {Aging},
doi = {10.1007/978-1-4615-5539-1},
eprint = {arXiv:1011.1669v3},
isbn = {0792382714},
issn = {19454589},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {254},
pmid = {25246403},
publisher = {Springer US},
series = {The Kluwer International Series in Engineering and Computer Science},
title = {{Information Retrieval: Algorithms and Heuristics}},
url = {https://books.google.com/books?id=JZnnUqIbpqAC{\&}pgis=1},
volume = {461},
year = {1998}
}
@article{Zesch2008,
abstract = {We introduce Wiktionary as an emerging lexical semantic re- source that can be used as a substitute for expert-made re- sources in AI applications. We evaluate Wiktionary on the pervasive task of computing semantic relatedness for English and German by means of correlation with human rankings and solving word choice problems. For the rst time, we ap- ply a concept vector based measure to a set of different con- cept representations like Wiktionary pseudo glosses, the rst paragraph of Wikipedia articles, English WordNet glosses, and GermaNet pseudo glosses. We show that: (i) Wiktionary is the best lexical semantic resource in the ranking task and performs comparably to other resources in the word choice task, and (ii) the concept vector based approach yields the best results on all datasets in both evaluations.},
author = {Zesch, Torsten and M{\"{u}}ller, Christof and Gurevych, Iryna},
file = {:home/bjorn/Downloads/AAAI08-137.pdf:pdf},
isbn = {9781577353683},
journal = {Proceedings of AAAI},
pages = {861--866},
title = {{Using Wiktionary for Computing Semantic Relatedness}},
volume = {8},
year = {2008}
}
@article{Liu2005,
abstract = {This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward-building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to demonstrate the use of feature selection in data mining. We conclude this work by identifying trends and challenges of feature selection research and development.},
author = {{Huan Liu} and {Lei Yu}},
doi = {10.1109/TKDE.2005.66},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huan Liu, Lei Yu - 2005 - Toward integrating feature selection algorithms for classification and clustering.pdf:pdf},
isbn = {1041-4347},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Categorizing framework,Classification,Clustering,Feature selection,Real-world applications,Unifying platform},
month = {apr},
number = {4},
pages = {491--502},
title = {{Toward integrating feature selection algorithms for classification and clustering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1401889},
volume = {17},
year = {2005}
}
@article{SEEWALD,
abstract = {In this paper, we discuss grading, a meta-classification technique that tries to identify and correct incorrect predictions at the base level. While stacking uses the predictions of the base classifiers as meta-level attributes, we use graded predictions (i.e., predictions that have been marked as correct or incorrect) as meta-level classes. For each base classifier, one meta classifier is learned whose task is to predict when the base classifier will err. Hence, just like stacking may be viewed as a generalization of voting, grading may be viewed as a generalization of selection by cross-validation and therefore fills a conceptual gap in the space of meta-classification schemes. Our experimental evaluation shows that this technique results in a performance gain that is quite comparable to that achieved by stacking, while both, grading and stacking outperform their simpler counter-parts voting and selection by cross-validation.},
author = {Seewald, AK and F{\"{u}}rnkranz, J},
doi = {10.1007/3-540-44816-0_12},
isbn = {3540425810},
issn = {16113349},
journal = {Advances in Intelligent Data Analysis. Lecture Notes in Computer Science},
keywords = {Algorithme apprentissage,Algoritmo aprendizaje,Clasificador,Classificateur,Classifier,Cross validation,Evaluaci{\'{o}}n prestaci{\'{o}}n,Evaluation performance,Learning algorithm,M{\'{e}}taclassification,Performance evaluation,Validaci{\'{o}}n cruzada,Validation crois{\'{e}}e,Vote,Voting,Voto},
pages = {115--124},
publisher = {Springer},
title = {{An evaluation of grading classifiers}},
url = {http://link.springer.com/chapter/10.1007/3-540-44816-0{\_}12},
volume = {2189},
year = {2001}
}
@article{Kraaij1994,
abstract = {... Taking these considerations into account, six rule clusters were created for the Dutch Porter stemmer . ... Paice has compared three English stemmers : Lovins, Paice/Husk and Porter with the truncate stemmer . The trunc(n) stemmer reduces a word to its first n characters. ...},
author = {Kraaij, Wessel and Pohlmann, Ren{\'{e}}e},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kraaij, Pohlmann - 1994 - Porter`s stemming algorithm for Dutch.pdf:pdf},
journal = {Informatiewetenschap 1994: Wetenschapelijke bijdragen aan de derde STINFON Conferentie},
pages = {167--180},
title = {{Porter`s stemming algorithm for Dutch}},
year = {1994}
}
@article{Rigsbergen1986,
abstract = {Implicit in many information retrieval model is logic. These logics are hardly ever formalised. This paper formalises non-classical logic underlying information retrieval. It shows how a particular conditional logic is the 'right' logic to do Information Retrieval. Its relationship to exisitng retrieval mechanisms is investigated. The semantics of the logic are expressed in probability theory, and evaluated through a possible-world analysis, thus establishing an intensional logic. In doing so, we motivate a new principle, the logic uncertainty principle, which gives a measure of the uncertainty associated with an inference.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rigsbergen, Van V. J.},
doi = {10.1093/comjnl/29.6.481},
eprint = {arXiv:1011.1669v3},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rigsbergen - 1986 - A non classical logic for Information retrieval.pdf:pdf},
isbn = {9780874216561},
issn = {0010-4620},
journal = {The Computer Journal},
keywords = {icle},
month = {jun},
number = {6},
pages = {481--485},
pmid = {25246403},
title = {{A non classical logic for Information retrieval}},
url = {http://comjnl.oupjournals.org/cgi/doi/10.1093/comjnl/29.6.481},
volume = {29},
year = {1986}
}
@article{Bennett2007,
abstract = {In October, 2006 Netflix released a dataset containing 100 million anonymous movie ratings and challenged the data mining, machine learning and computer science communities to develop systems that could beat the accuracy of its recommendation system, Cinematch. We briefly describe the challenge itself, review related work and efforts, and summarize visible progress to date. Other potential uses of the data are outlined, including its application to the KDD Cup 2007.},
author = {Bennett, James and Lanning, Stan},
doi = {10.1145/1562764.1562769},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bennett, Lanning - 2007 - The Netflix Prize.pdf:pdf},
isbn = {9781595938343},
issn = {1554351X},
journal = {KDD Cup and Workshop},
keywords = {machine learning,netflix prize,rmse},
pages = {3--6},
title = {{The Netflix Prize}},
url = {http://su-2010-projekt.googlecode.com/svn-history/r157/trunk/literatura/bennett2007netflix.pdf},
year = {2007}
}
@article{McNamee2004,
abstract = {The caliber of available translation resources is a key factor in the accuracy of a system for - information retrieval ( and 2002a},
author = {McNamee, Paul and Mayfield, James},
doi = {10.1023/B:INRT.0000009441.78971.be},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McNamee, Mayfield - 2004 - Character N-Gram Tokenization for European Language Text Retrieval.pdf:pdf},
issn = {1386-4564},
journal = {Information Retrieval},
keywords = {character n -grams,cross language,cross-language information retrieval,european languages,evaluation forum,language-neutral retrieval},
number = {1-2},
pages = {73--97},
title = {{Character N-Gram Tokenization for European Language Text Retrieval}},
url = {http://www.springerlink.com/openurl.asp?id=doi:10.1023/B:INRT.0000009441.78971.be},
volume = {7},
year = {2004}
}
@article{Kittler1998,
author = {Kittler, Josef and Society, Ieee Computer and Hatef, Mohamad and Duin, Robert P W and Matas, Jiri},
doi = {10.1109/34.667881},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kittler et al. - 1998 - On Combining Classifiers.pdf:pdf},
isbn = {081867282X},
issn = {10514651},
number = {3},
pages = {226--239},
pmid = {20470429},
title = {{On Combining Classifiers}},
volume = {20},
year = {1998}
}
@article{Weale2009,
author = {Weale, Timothy and Brew, Chris and Fosler-lussier, Eric},
doi = {10.3115/1699765.1699769},
file = {:home/bjorn/Downloads/p28-weale.pdf:pdf},
isbn = {1932432558},
journal = {In Proceedings of the Workshop on the People's Web Meets NLP},
number = {August},
pages = {28--31},
title = {{Using the wiktionary graph structure for synonym detection}},
year = {2009}
}
@article{Everything2012,
author = {Linden, G. and Smith, B. and York, J.},
doi = {10.1109/MIC.2003.1167344},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Linden, Smith, York - 2003 - Amazon.com recommendations item-to-item collaborative filtering.pdf:pdf},
issn = {1089-7801},
journal = {IEEE Internet Computing},
month = {jan},
number = {1},
pages = {76--80},
title = {{Amazon.com recommendations: item-to-item collaborative filtering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1167344},
volume = {7},
year = {2003}
}
@article{Salton1988,
author = {Salton, Gerard and Buckley, Christopher},
doi = {10.1016/0306-4573(88)90021-0},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Buckley - 1988 - Term-weighting approaches in automatic text retrieval.pdf:pdf},
issn = {03064573},
journal = {Information Processing {\&} Management},
month = {jan},
number = {5},
pages = {513--523},
title = {{Term-weighting approaches in automatic text retrieval}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0306457388900210},
volume = {24},
year = {1988}
}
@article{Salton1975,
author = {Salton, G and Wong, A and Yang, C S},
doi = {http://doi.acm.org/10.1145/361219.361220},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Wong, Yang - 1975 - A vector space model for automatic indexing.pdf:pdf},
issn = {0001-0782},
journal = {Cacm},
keywords = {automatic indexing,automatic information retrieval,content analysis,document space},
number = {11},
pages = {613--620},
title = {{A vector space model for automatic indexing}},
url = {http://doi.acm.org/10.1145/361219.361220},
volume = {18},
year = {1975}
}
@article{Ceska2009,
abstract = {This paper explores the influence of text pre- processing techniques on plagiarism detection. We examine stop-word removal, lemmatization, number replacement, synonymy recognition, and word generalization. We also look into the in- fluence of punctuation and word-order within N-grams. All these techniques are evaluated according to their impact on F1-measure and speed of execution. Our experiments were per- formed on a Czech corpus of plagiarized docu- ments about politics. At the end of this paper, we propose what we consider to be the best com- bination of text pre-processing techniques.},
author = {Ceska, Zdenek and Fox, Chris},
file = {:home/bjorn/Downloads/R09-1011.pdf:pdf},
issn = {13138502},
journal = {International Conference RANLP 2009},
keywords = {copy detection,lemmatization,natural language processing,plagiarism,stop-words,synonymy,thesaurus,wordnet},
pages = {55--59},
title = {{The Influence of Text Pre-processing on Plagiarism Detection}},
year = {2009}
}
@article{Hotho2005,
abstract = {The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.},
author = {Hotho, Andreas and N{\"{u}}rnberger, Andreas and Paa{\ss}, Gerhard},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hotho, N{\"{u}}rnberger, Paa{\ss} - 2005 - A Brief Survey of Text Mining(2).pdf:pdf},
issn = {0175-1336},
journal = {LDV-Forum},
keywords = {Survey,Text Mining},
number = {1},
pages = {19--62},
title = {{A Brief Survey of Text Mining}},
volume = {20},
year = {2005}
}
@article{Science2013,
author = {Science, Computer and Studies, Management},
file = {:home/bjorn/Downloads/V1I6-0015.pdf:pdf},
keywords = {akaike,bayesian inference criterion,clustering,cross-validation,elbow method,jump,method,number of cluster,s information criterion,silhouette},
number = {6},
pages = {90--95},
title = {{Review on determining number of Cluster in K-Means Clustering}},
volume = {1},
year = {2013}
}
@article{Nasa2012,
author = {Nasa, Divya},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nasa - 2012 - Text Mining Techniques- A Survey.pdf:pdf},
keywords = {applications,benefits and limitations,classification,clustering,information extraction,summarization,techniques,text mining framework,visualization},
number = {4},
pages = {1--5},
title = {{Text Mining Techniques- A Survey}},
volume = {2},
year = {2012}
}
@misc{Porter1980,
abstract = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL....},
author = {Porter, M.F.},
booktitle = {Program: electronic library and information systems},
doi = {10.1108/eb046814},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Porter - 1980 - An algorithm for suffix stripping.pdf:pdf},
isbn = {1558604545},
issn = {0033-0337},
number = {3},
pages = {130--137},
pmid = {16143652},
title = {{An algorithm for suffix stripping}},
volume = {14},
year = {1980}
}
@misc{ROBERTSON1977,
abstract = {The principle that, for optimal retrieval, documents should be ranked in order of the probability of relevance or usefulness has been brought into question by Cooper. It is shown that the principle can be justified under certain assumptions, but that in cases where these assumptions do not hold, the principle is not valid. The major problem appears to lie in the way the principle considers each document independently of the rest. The nature of the information on the basis of which the system decides whether or not to retrieve the documents determines whether the document-by-document approach is valid.},
author = {ROBERTSON, S.E.},
booktitle = {Journal of Documentation},
doi = {10.1108/eb026647},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ROBERTSON - 1977 - the Probability Ranking Principle in Ir.pdf:pdf},
isbn = {1558604545},
issn = {0022-0418},
number = {4},
pages = {294--304},
title = {{the Probability Ranking Principle in Ir}},
url = {http://www.emeraldinsight.com/doi/abs/10.1108/eb026647},
volume = {33},
year = {1977}
}
@book{Mattmann2011,
abstract = {Title: Tika In Action SubTitle: ; Volume: ; Serie: ; Edition: ; Authors: Mattmann, Chris A. And Zitting, Jukka L. ; Year: 2011 ; Pages: 257 ; Editor: ; Publisher: Manning ; ISBN: 978-1-234-56789-7 ; Keywords: Action; Tika ;},
author = {Mattmann, Chris a and Zitting, Jukka L},
booktitle = {Tika In Action},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mattmann, Zitting - 2011 - Tika In Action.pdf:pdf},
isbn = {978-1-234-56789-7},
keywords = {Action,Tika},
pages = {257},
title = {{Tika In Action}},
year = {2011}
}
@article{Sukanya2012,
abstract = {Knowledge discovery from textual database refers generally to the process of extracting interesting or non-retrieval patterns or knowledge from unstructured text documents. This is also called as text data mining or knowledge discovery. It can be viewed as an extension of data mining or knowledge from structured databases. At present, the stored information is increasing tremendously day by day. This is unstructured form so we can not extract the needed information. Some data mining techniques are used to extract the useful information from text documents, such as classification, clustering, visualization and information extraction. Here framework of text mining with techniques is discussed as well as benefits and limitations of text mining have been discussed.},
author = {Sukanya, M. and Biruntha, S.},
doi = {10.1109/ICACCCT.2012.6320784},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sukanya, Biruntha - 2012 - Techniques on text mining.pdf:pdf},
isbn = {9781467320467},
journal = {Proceedings of 2012 IEEE International Conference on Advanced Communication Control and Computing Technologies, ICACCCT 2012},
keywords = {Text mining framework,benefits and limitations,classification,clustering,information extraction,visualization},
number = {978},
pages = {269--271},
title = {{Techniques on text mining}},
year = {2012}
}
@article{Sigletos2005,
author = {Sigletos, Georgios and Paliouras, Georgios and Spyropoulos, Constantine D and Hatzopoulos, Michalis},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spyropoulos - 2005 - Combining Information Extraction Systems Using Voting and Stacked Generalization Georgios Sigletos Georgios Paliour.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,information extraction,stacking,voting},
pages = {1751--1782},
title = {{Combining Information Extraction Systems Using Voting and Stacked Generalization Georgios Sigletos Georgios Paliouras}},
volume = {6},
year = {2005}
}
@article{Forman2003,
abstract = {Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives—accuracy, F-measure, precision, and recall—since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair—e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.},
author = {Forman, George},
doi = {10.1162/153244303322753670},
file = {:home/bjorn/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Forman - 2003 - An Extensive Empirical Study of Feature Selection Metrics for Text Classification.pdf:pdf},
isbn = {1558604863},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {ROC,document categorization,supervised learning,support vector machines},
pages = {1289--1305},
title = {{An Extensive Empirical Study of Feature Selection Metrics for Text Classification}},
volume = {3},
year = {2003}
}
@article{Dash1997,
abstract = {Feature selection has been the focus of interest for quite some time and much work has been done.With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications.},
author = {Dash, M. and Liu, H.},
doi = {10.3233/IDA-1997-1302},
file = {:home/bjorn/Downloads/dash-1997.pdf:pdf},
isbn = {1088-467X},
issn = {1088467X},
journal = {Intelligent Data Analysis},
keywords = {Classification,Feature selection,Framework},
number = {3},
pages = {131--156},
pmid = {21600290},
title = {{Feature selection for classification}},
volume = {1},
year = {1997}
}
@article{Fayyad1996,
abstract = {This paper presents a first step toward a unifying framework for Knowldege Discovery in Databases. We describe links between data mining,knowledge discovery and other related fields. We then define the KDD process and basic data mining algorithms, discuss application issues and conclude with an analysis of challenges facing practitioners in the field},
author = {Fayyad, UM and Piatetsky-Shapiro, G and Smyth, P},
doi = {10.1.1.27.363},
file = {:home/bjorn/Downloads/document.pdf:pdf},
isbn = {1-57735-004-9},
journal = {Proc 2nd Int Conf on Knowledge Discovery and Data Mining Portland OR},
pages = {82--88},
title = {{Knowledge Discovery and Data Mining: Towards a Unifying Framework.}},
url = {http://www.aaai.org/Papers/KDD/1996/KDD96-014},
year = {1996}
}
